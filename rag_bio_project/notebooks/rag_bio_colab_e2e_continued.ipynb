{"cells":[{"cell_type":"markdown","id":"tEgHi1sbaSrQ","metadata":{"id":"tEgHi1sbaSrQ"},"source":["# 📘 RAG Biography (Colab Edition)\n","\n","**One-click end-to-end:** install deps → mount Drive → set project path → generate sample PDFs → load → split → embed → build Chroma collections `(username)_(character_name)` → quick retrieval.\n","\n","> Default embeddings: **HuggingFace BGE small zh v1.5** (no API needed). You can switch to OpenAI/DashScope by editing the *Embeddings* cell.\n"]},{"cell_type":"code","execution_count":1,"id":"rXkdWBCNaSrR","metadata":{"id":"rXkdWBCNaSrR","executionInfo":{"status":"ok","timestamp":1759731868365,"user_tz":300,"elapsed":12,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["# # # ===== Clean + Pin =====\n","# import sys, subprocess, IPython\n","\n","# def sh(cmd):\n","#     print(\">>\", cmd)\n","#     subprocess.run(cmd, shell=True, check=False)\n","\n","# # 1) 清 spacy 家族（会拉 weasel/srsly/wasabi，常搅版本）\n","# sh(\"pip -q uninstall -y spacy thinc catalogue srsly cymem preshed murmurhash wasabi blis typer langcodes || true\")\n","\n","# # 2) 固定 requests（避免和系统包冲突）\n","# sh(\"pip -q install -U requests==2.32.2\")\n","\n","# # 3) 关键：强制把 numpy 降到 1.26.4（并避免再次被别的包升回去）\n","# sh('pip -q install --force-reinstall --no-build-isolation \"numpy==1.26.4\"')\n","\n","# # 4) 你的依赖\n","# sh(\"pip -q install -U chromadb==0.4.24 \"\n","#    \"langchain==0.2.11 langchain-core==0.2.26 langchain-community==0.2.10 \"\n","#    \"langchain-openai==0.1.17 pypdf tiktoken\")\n","\n","# # 5) 校验依赖冲突（可选）\n","# sh(\"pip -q check || true\")\n","\n","# sh(\"pip -q install -U reportlab python-dotenv\")\n","\n","# print(\"\\n✅ 安装完成：即将重启内核以切换到 NumPy 1.26.4 ...\")\n","# IPython.get_ipython().kernel.do_shutdown(restart=True)  # 自动重启\n"]},{"cell_type":"code","execution_count":2,"id":"v6qY0wVfaSrR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6qY0wVfaSrR","outputId":"11ca3d8b-bb79-49f3-a9bc-67498c09ce23","executionInfo":{"status":"ok","timestamp":1759731868957,"user_tz":300,"elapsed":578,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Project path: /content/drive/MyDrive/rag_bio_project\n","✅ Folders ready.\n"]}],"source":["from google.colab import drive\n","from pathlib import Path\n","drive.mount('/content/drive')\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","print('Project path:', PROJ)\n","\n","for d in [PROJ, PROJ/'src', PROJ/'data_pdfs', PROJ/'data_txt', PROJ/'index', PROJ/'notebooks']:\n","    d.mkdir(parents=True, exist_ok=True)\n","print('✅ Folders ready.')"]},{"cell_type":"code","execution_count":3,"id":"VUOy578IaSrR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUOy578IaSrR","outputId":"0caa3822-7a16-4206-d68a-3d382a3165e8","executionInfo":{"status":"ok","timestamp":1759731868972,"user_tz":300,"elapsed":14,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[SKIP ] /content/drive/MyDrive/rag_bio_project/src/loader.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/splitter.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/embeddings.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/vectorstore.py\n","✅ src/ files ensured.\n"]}],"source":["from pathlib import Path\n","import textwrap\n","\n","SRC = PROJ/'src'\n","\n","def ensure_file(path: Path, content: str):\n","    if not path.exists():\n","        path.write_text(textwrap.dedent(content), encoding='utf-8')\n","        print('[WRITE]', path)\n","    else:\n","        print('[SKIP ]', path)\n","\n","# --- loader.py ---\n","ensure_file(SRC/'loader.py', '''\n","from pathlib import Path\n","from typing import List, Optional, Iterable, Dict\n","import hashlib, re, time\n","from urllib.parse import urlsplit, urlunsplit\n","from langchain_core.documents import Document\n","from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n","def _normalize_text(s: str) -> str:\n","    if not s: return ''\n","    s = s.replace('\\ufeff', '').replace('\\xa0', ' ')\n","    s = re.sub(r'[ \\t]+', ' ', s)\n","    s = re.sub(r'\\n{3,}', '\\n\\n', s)\n","    return s.strip()\n","def _clean_url(u: str) -> str:\n","    u = u.strip().replace(' ', '')\n","    parts = list(urlsplit(u))\n","    if not parts[0]: parts[0] = 'https'\n","    return urlunsplit(parts)\n","def _doc_hash(content: str) -> str:\n","    return hashlib.md5(content.encode('utf-8', errors='ignore')).hexdigest()[:16]\n","def _filter_content(text: str, min_chars: int, max_chars: Optional[int]) -> bool:\n","    n = len(text)\n","    if n < min_chars: return False\n","    if max_chars is not None and n > max_chars: return False\n","    return True\n","def load_sources(pdf_dir: str='data_pdfs', txt_dir: Optional[str]='data_txt', urls: Optional[List[str]] = None,\n","                 recursive: bool=True, txt_extensions: Iterable[str]=( '.txt', '.md'), txt_encoding: str='utf-8',\n","                 pages: Optional[str]=None, min_chars: int=50, max_chars: Optional[int]=None,\n","                 headers: Optional[Dict[str,str]] = None, timeout: int=15, max_retries: int=2) -> List[Document]:\n","    docs: List[Document] = []\n","    total_raw = 0\n","    pdir = Path(pdf_dir)\n","    if pdir.exists():\n","        for p in (pdir.rglob('*.pdf') if recursive else pdir.glob('*.pdf')):\n","            try:\n","                loader = PyPDFLoader(str(p))\n","                loaded = loader.load()\n","                if pages:\n","                    parts = [None if x in ('', 'None', None) else int(x) for x in pages.split(':')]\n","                    start = parts[0] if len(parts)>0 else None\n","                    stop  = parts[1] if len(parts)>1 else None\n","                    step  = parts[2] if len(parts)>2 else None\n","                    loaded = [d for d in loaded[slice(start, stop, step)]]\n","                for d in loaded:\n","                    d.page_content = _normalize_text(d.page_content)\n","                    d.metadata['source'] = str(p)\n","                    d.metadata['source_type'] = 'pdf'\n","                    d.metadata['loader_info'] = {'type':'PyPDFLoader','pages':pages}\n","                    if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                total_raw += len(loaded)\n","            except Exception as e:\n","                print(f'[WARN] Skip PDF {p}: {e}')\n","    if txt_dir:\n","        tdir = Path(txt_dir)\n","        if tdir.exists():\n","            exts = {e.lower() for e in txt_extensions}\n","            for p in (tdir.rglob('*') if recursive else tdir.glob('*')):\n","                if p.is_file() and p.suffix.lower() in exts:\n","                    try:\n","                        loader = TextLoader(str(p), encoding=txt_encoding)\n","                        loaded = loader.load()\n","                        for d in loaded:\n","                            d.page_content = _normalize_text(d.page_content)\n","                            d.metadata['source'] = str(p)\n","                            d.metadata['source_type'] = 'txt'\n","                            d.metadata['loader_info'] = {'type':'TextLoader','encoding':txt_encoding}\n","                            if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                        total_raw += len(loaded)\n","                    except Exception as e:\n","                        print(f'[WARN] Skip TXT {p}: {e}')\n","    if urls:\n","        urls = [u for u in (urls or []) if isinstance(u, str) and u.strip()]\n","        for u in urls:\n","            url = _clean_url(u)\n","            tries = 0\n","            while True:\n","                try:\n","                    loader = WebBaseLoader(url, requests_kwargs={'headers': headers or {'User-Agent':'Mozilla/5.0 (RAG-Loader/1.0)'}, 'timeout': timeout})\n","                    loaded = loader.load()\n","                    for d in loaded:\n","                        d.page_content = _normalize_text(d.page_content)\n","                        d.metadata['source'] = url\n","                        d.metadata['source_type'] = 'web'\n","                        d.metadata['loader_info'] = {'type':'WebBaseLoader','timeout':timeout,'headers':bool(headers)}\n","                        if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                    total_raw += len(loaded)\n","                    break\n","                except Exception as e:\n","                    tries += 1\n","                    if tries > max_retries:\n","                        print(f'[WARN] Skip URL {url} after {max_retries} retries: {e}'); break\n","                    print(f'[INFO] Retry {tries}/{max_retries} for URL {url} due to: {e}')\n","    seen = set(); uniq: List[Document] = []\n","    for d in docs:\n","        h = _doc_hash(d.page_content)\n","        key = (d.metadata.get('source'), h)\n","        if key not in seen:\n","            seen.add(key); uniq.append(d)\n","    print(f'[INFO] Loader complete. raw={total_raw}, kept={len(uniq)}')\n","    return uniq\n","''')\n","\n","# --- splitter.py ---\n","ensure_file(SRC/'splitter.py', '''\n","from typing import List, Dict\n","from dataclasses import dataclass\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","@dataclass\n","class SplitterProfile:\n","    chunk_size: int; chunk_overlap: int; separators: List[str]\n","PDF_PROFILE = SplitterProfile(1200, 200, ['\\\\n\\\\n','\\\\n','。','！','？','.', '!', '?', ' ', ''])\n","TXT_PROFILE = SplitterProfile(1000, 150, ['\\\\n\\\\n','\\\\n','.', '?', '!', '。','？','！',' ', ''])\n","WEB_PROFILE = SplitterProfile(900, 150,  ['\\\\n\\\\n','\\\\n','。','！','？','.', '!', '?', ' ', ''])\n","PROFILE_MAP: Dict[str, SplitterProfile] = {'pdf':PDF_PROFILE,'txt':TXT_PROFILE,'web':WEB_PROFILE}\n","def _build_splitter(p: SplitterProfile) -> RecursiveCharacterTextSplitter:\n","    overlap = p.chunk_overlap if p.chunk_overlap < p.chunk_size else max(0, min(p.chunk_size//5, 200))\n","    if overlap != p.chunk_overlap: print(f'[WARN] overlap >= size; fallback to {overlap}')\n","    return RecursiveCharacterTextSplitter(chunk_size=p.chunk_size, chunk_overlap=overlap, separators=p.separators)\n","def split_documents_type_aware(docs: List[Document], default_type: str='pdf', verbose: bool=True) -> List[Document]:\n","    if verbose: print(f'[INFO] Type-aware splitting started. total_docs={len(docs)}, default_type={default_type}')\n","    splitter_cache: Dict[str, RecursiveCharacterTextSplitter] = {}\n","    def get_splitter(kind: str):\n","        typ = (kind or '').lower(); typ = typ if typ in PROFILE_MAP else default_type\n","        if typ not in splitter_cache:\n","            splitter_cache[typ] = _build_splitter(PROFILE_MAP[typ])\n","            if verbose:\n","                p = PROFILE_MAP[typ]; print(f\"[INFO] Splitter ready for type='{typ}' (size={p.chunk_size}, overlap={p.chunk_overlap})\")\n","        return splitter_cache[typ]\n","    out: List[Document] = []; per_source_index: Dict[str,int] = {}\n","    for d in docs:\n","        stype = (d.metadata.get('source_type') or default_type).lower()\n","        splitter = get_splitter(stype)\n","        chunks = splitter.split_documents([d])\n","        source_key = str(d.metadata.get('source', 'unknown'))\n","        start_idx = per_source_index.get(source_key, 0)\n","        for i, c in enumerate(chunks):\n","            c.metadata = dict(d.metadata) | {'chunk_index': start_idx + i, 'splitter_profile': stype}\n","            out.append(c)\n","        per_source_index[source_key] = start_idx + len(chunks)\n","        if verbose: print(f\"[INFO] Split {stype} source={source_key} -> {len(chunks)} chunks (acc={len(out)})\")\n","    if verbose: print(f'[INFO] Type-aware splitting done. total_chunks={len(out)}')\n","    return out\n","''')\n","\n","# --- embeddings.py ---\n","ensure_file(SRC/'embeddings.py', '''\n","import os, re, time, hashlib\n","from dataclasses import dataclass\n","from typing import List, Tuple, Dict, Optional\n","from langchain_core.documents import Document\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.embeddings import DashScopeEmbeddings\n","from chromadb import PersistentClient\n","@dataclass\n","class EmbeddingConfig:\n","    provider: str = 'hf'\n","    model: str = 'BAAI/bge-small-zh-v1.5'\n","    normalize: bool = True\n","def _sanitize_name(s: str, fallback_prefix: str='user') -> str:\n","    s2 = re.sub(r'[^a-zA-Z0-9_]+', '_', str(s or '')).strip('_')\n","    return s2 or f\"{fallback_prefix}_{int(time.time())}\"\n","def _doc_id(meta: Dict, idx: int) -> str:\n","    base = f\"{meta.get('source','unknown')}|{meta.get('chunk_index', idx)}\"\n","    return hashlib.md5(base.encode('utf-8', errors='ignore')).hexdigest()[:12] + f'_{idx}'\n","def build_embeddings(cfg: EmbeddingConfig):\n","    prov = cfg.provider.lower()\n","    if prov == 'hf':\n","        print(f\"[INFO] Using HF embeddings: {cfg.model} (normalize={cfg.normalize})\")\n","        return HuggingFaceBgeEmbeddings(model_name=cfg.model, encode_kwargs={'normalize_embeddings': cfg.normalize})\n","    elif prov == 'openai':\n","        if not os.getenv('OPENAI_API_KEY'): raise RuntimeError('OPENAI_API_KEY is not set.')\n","        model_name = cfg.model or 'text-embedding-3-small'\n","        print(f\"[INFO] Using OpenAI embeddings: {model_name}\")\n","        return OpenAIEmbeddings(model=model_name)\n","    elif prov == 'dashscope':\n","        if not os.getenv('DASHSCOPE_API_KEY'): raise RuntimeError('DASHSCOPE_API_KEY is not set.')\n","        model_name = cfg.model or 'text-embedding-v1'\n","        print(f\"[INFO] Using DashScope embeddings: {model_name}\")\n","        return DashScopeEmbeddings(model=model_name)\n","    else:\n","        raise ValueError(f'Unknown provider: {cfg.provider}')\n","def compute_vectors_once(chunks: List[Document], embedder):\n","    texts = [c.page_content for c in chunks]\n","    metas = [dict(c.metadata or {}) for c in chunks]\n","    ids = [_doc_id(m, i) for i, m in enumerate(metas)]\n","    print(f\"[INFO] Computing embeddings for {len(texts)} chunks...\")\n","    vectors = embedder.embed_documents(texts)\n","    if not vectors or not vectors[0]: raise RuntimeError('Empty embeddings returned.')\n","    dim = len(vectors[0])\n","    emb_info = {'provider': type(embedder).__name__, 'dim': dim}\n","    print(f\"[INFO] Embedding dim={dim}\")\n","    return ids, texts, metas, vectors, emb_info\n","def persist_vectorstores_for_characters(ids, texts, metas, vectors, persist_dir, username, character_names, emb_meta, collection_prefix: Optional[str]=None):\n","    user_tag = _sanitize_name(username, 'user')\n","    created = []\n","    client: PersistentClient = PersistentClient(path=persist_dir)\n","    for cname in character_names:\n","        char_tag = _sanitize_name(cname, 'char')\n","        base_name = f\"{user_tag}_{char_tag}\"\n","        coll_name = base_name if not collection_prefix else _sanitize_name(f\"{collection_prefix}_{base_name}\", 'coll')\n","        print(f\"[INFO] Creating/updating collection: {coll_name}\")\n","        coll = client.get_or_create_collection(name=coll_name, metadata={'embedding': emb_meta})\n","        B = 256\n","        for i in range(0, len(ids), B):\n","            coll.add(ids=ids[i:i+B], documents=texts[i:i+B], metadatas=metas[i:i+B], embeddings=vectors[i:i+B])\n","        print(f\"[INFO] Collection '{coll_name}' upserted with {len(ids)} items.\")\n","        created.append(coll_name)\n","    print(f\"[INFO] Done. Created/updated {len(created)} collections.\")\n","    return created\n","def build_embeddings_and_vectorstores(chunks: List[Document], username: str, character_names: List[str], persist_dir: str='index', emb_cfg: EmbeddingConfig = EmbeddingConfig()):\n","    if not character_names: raise ValueError('character_names must not be empty.')\n","    embedder = build_embeddings(emb_cfg)\n","    ids, texts, metas, vectors, emb_info = compute_vectors_once(chunks, embedder)\n","    return persist_vectorstores_for_characters(ids, texts, metas, vectors, persist_dir, username, character_names, {'provider':emb_cfg.provider,'model':emb_cfg.model,'dim':emb_info['dim'],'normalize':emb_cfg.normalize})\n","''')\n","\n","# --- vectorstore.py ---\n","ensure_file(SRC/'vectorstore.py', '''\n","from chromadb import PersistentClient\n","def get_collection(persist_dir: str, collection_name: str):\n","    client = PersistentClient(path=persist_dir)\n","    return client.get_collection(collection_name)\n","def quick_query(persist_dir: str, collection_name: str, query_text: str, n_results: int=5):\n","    coll = get_collection(persist_dir, collection_name)\n","    return coll.query(query_texts=[query_text], n_results=n_results, include=['documents','metadatas','distances'])\n","''')\n","\n","print('✅ src/ files ensured.')"]},{"cell_type":"code","execution_count":4,"id":"bqtMUD1CaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqtMUD1CaSrS","outputId":"c16a1aa9-bd6a-48be-98fd-b15f867cbab2","executionInfo":{"status":"ok","timestamp":1759731875074,"user_tz":300,"elapsed":6097,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Added to sys.path: /content/drive/MyDrive/rag_bio_project/src\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]},{"output_type":"stream","name":"stdout","text":["✅ Imports OK\n"]}],"source":["import sys\n","from pathlib import Path\n","CANDIDATES = [PROJ, Path.cwd(), Path('/content/drive/MyDrive/rag_bio_project')]\n","project_src=None\n","for base in CANDIDATES:\n","    src = base/'src'\n","    if src.exists() and (src/'loader.py').exists():\n","        project_src=src.resolve(); break\n","assert project_src, 'src/ not found. Check PROJ path.'\n","sys.path.insert(0, str(project_src))\n","print('[INFO] Added to sys.path:', project_src)\n","\n","from loader import load_sources\n","from splitter import split_documents_type_aware\n","from embeddings import EmbeddingConfig, build_embeddings_and_vectorstores\n","from vectorstore import quick_query\n","print('✅ Imports OK')"]},{"cell_type":"code","execution_count":5,"id":"B9t-jeBCaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9t-jeBCaSrS","outputId":"e8e449de-b225-4bd6-aa2a-9c9aefdfa552","executionInfo":{"status":"ok","timestamp":1759731876751,"user_tz":300,"elapsed":1665,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] ENV loaded. OPENAI_API_KEY exists: False\n","[INFO] PDFs generated at /content/drive/MyDrive/rag_bio_project/data_pdfs\n","[INFO] TXT samples ensured at data_txt\n"]}],"source":["from reportlab.lib.pagesizes import letter\n","from reportlab.pdfgen import canvas\n","from dotenv import load_dotenv\n","import os\n","\n","DATA_PDFS = PROJ/'data_pdfs'\n","DATA_TXT  = PROJ/'data_txt'\n","INDEX_DIR = PROJ/'index'\n","ENV_PATH  = PROJ/' .env'\n","load_dotenv(ENV_PATH)\n","print('[INFO] ENV loaded. OPENAI_API_KEY exists:', bool(os.getenv('OPENAI_API_KEY')))\n","\n","def make_pdf(path, title, lines):\n","    c = canvas.Canvas(str(path), pagesize=letter)\n","    width, height = letter\n","    y = height - 72\n","    c.setFont('Times-Roman', 12)\n","    c.drawString(72, y, title); y -= 24\n","    for ln in lines:\n","        for seg in ln.split('\\n'):\n","            c.drawString(72, y, seg); y -= 18\n","            if y < 72:\n","                c.showPage(); y = height - 72; c.setFont('Times-Roman', 12)\n","    c.save()\n","\n","make_pdf(DATA_PDFS/'liqing.pdf', 'Li Qing Biography', [\n","    'Born in Suzhou in 1990; studied Economics and became a product manager.',\n","    'Finance: ~350k CNY income; index funds; medium risk preference.',\n","    'Marriage: married in 2018; one daughter in 2022.',\n","    'Experience: 2020-2022 SEA market localization projects.'\n","])\n","make_pdf(DATA_PDFS/'wangmu.pdf', 'Wang Mu Biography', [\n","    'Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations.',\n","    'Finance: ~200k CNY income; house + money-market funds; conservative.',\n","    'Marriage: single; passionate about education equity.',\n","    'Experience: 2015-2019 broker risk control; compliance expertise.'\n","])\n","print('[INFO] PDFs generated at', DATA_PDFS)\n","\n","from pathlib import Path\n","\n","DATA_TXT = Path(\"data_txt\")\n","DATA_TXT.mkdir(parents=True, exist_ok=True)\n","\n","liqing = \"\\n\".join([\n","    \"Born in Suzhou in 1990; studied Economics and became a product manager.\",\n","    \"Finance: ~350k CNY income; index funds; medium risk preference.\",\n","    \"Marriage: married in 2018; one daughter in 2022.\",\n","    \"Experience: 2020-2022 SEA market localization projects.\",\n","])\n","(DATA_TXT / \"liqing.txt\").write_text(liqing, encoding=\"utf-8\")\n","\n","wangmu = \"\\n\".join([\n","    \"Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations.\",\n","    \"Finance: ~200k CNY income; house + money-market funds; conservative.\",\n","    \"Marriage: single; passionate about education equity.\",\n","    \"Experience: 2015-2019 broker risk control; compliance expertise.\",\n","])\n","(DATA_TXT / \"wangmu.txt\").write_text(wangmu, encoding=\"utf-8\")\n","\n","print('[INFO] TXT samples ensured at', DATA_TXT)"]},{"cell_type":"code","execution_count":6,"id":"edEvIZTUeyMj","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edEvIZTUeyMj","outputId":"75e966d8-1590-4fda-c36b-fe43dbbc8aca","executionInfo":{"status":"ok","timestamp":1759731876778,"user_tz":300,"elapsed":28,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["✔ loader.py patched\n","✔ embeddings.py patched\n","✅ Patch applied & modules reloaded.\n"]}],"source":["from pathlib import Path\n","import importlib, re, json, sys\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","SRC = PROJ/'src'\n","assert SRC.exists(), f\"src not found: {SRC}\"\n","\n","# 1) patch loader.py: make loader_info a JSON string (not a dict)\n","lp = SRC/'loader.py'\n","ls = lp.read_text(encoding='utf-8')\n","\n","if 'import json' not in ls:\n","    ls = ls.replace('from urllib.parse import urlsplit, urlunsplit',\n","                    'from urllib.parse import urlsplit, urlunsplit\\nimport json')\n","\n","# PyPDFLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'PyPDFLoader','pages':pages}, ensure_ascii=False)\",\n","    ls\n",")\n","# TextLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'TextLoader','encoding':txt_encoding}, ensure_ascii=False)\",\n","    ls,\n","    count=1  # only replace the TXT occurrence once\n",")\n","# WebBaseLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'WebBaseLoader','timeout':timeout,'headers':bool(headers)}, ensure_ascii=False)\",\n","    ls,\n","    count=1  # only replace the Web occurrence once\n",")\n","\n","lp.write_text(ls, encoding='utf-8')\n","print(\"✔ loader.py patched\")\n","\n","# 2) patch embeddings.py: (a) collection metadata -> JSON string (b) sanitize per-doc metadatas before add()\n","ep = SRC/'embeddings.py'\n","es = ep.read_text(encoding='utf-8')\n","\n","if 'import json' not in es:\n","    es = es.replace('from chromadb import PersistentClient',\n","                    'from chromadb import PersistentClient\\nimport json')\n","\n","# add a sanitizer\n","if '_sanitize_meta_for_chroma' not in es:\n","    es = es.replace(\n","        'def compute_vectors_once(chunks: List[Document], embedder):',\n","        \"\"\"def _sanitize_meta_for_chroma(m: dict) -> dict:\n","    out = {}\n","    for k, v in (m or {}).items():\n","        if isinstance(v, (str, int, float, bool)):\n","            out[k] = v\n","        else:\n","            out[k] = json.dumps(v, ensure_ascii=False)\n","    return out\n","\n","def compute_vectors_once(chunks: List[Document], embedder):\"\"\"\n","    )\n","\n","# ensure collection metadata uses JSON string\n","es = es.replace(\n","    \"metadata={'embedding': emb_meta}\",\n","    \"metadata={'embedding': json.dumps(emb_meta, ensure_ascii=False)}\"\n",")\n","\n","# sanitize metas list before coll.add\n","es = re.sub(\n","    r\"for i in range\\(0, len\\(ids\\), B\\):\\s*\"\n","    r\"coll\\.add\\(ids=ids\\[i:i\\+B\\], documents=texts\\[i:i\\+B\\], metadatas=metas\\[i:i\\+B\\], embeddings=vectors\\[i:i\\+B\\]\\)\",\n","    \"for i in range(0, len(ids), B):\\n\"\n","    \"            batch_metas = [ _sanitize_meta_for_chroma(m) for m in metas[i:i+B] ]\\n\"\n","    \"            coll.add(ids=ids[i:i+B], documents=texts[i:i+B], metadatas=batch_metas, embeddings=vectors[i:i+B])\",\n","    es\n",")\n","\n","ep.write_text(es, encoding='utf-8')\n","print(\"✔ embeddings.py patched\")\n","\n","# 3) reload modules\n","sys.path.insert(0, str(SRC))\n","import loader, embeddings\n","import importlib\n","importlib.reload(loader)\n","importlib.reload(embeddings)\n","from embeddings import EmbeddingConfig, build_embeddings_and_vectorstores\n","print(\"✅ Patch applied & modules reloaded.\")\n"]},{"cell_type":"code","execution_count":7,"id":"aEENRsoyaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":967},"id":"aEENRsoyaSrS","outputId":"d6793154-1c7e-4b1c-b14e-a8f15b329c5d","executionInfo":{"status":"error","timestamp":1759731894658,"user_tz":300,"elapsed":17878,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Loader complete. raw=4, kept=4, pdf_dir=/content/drive/MyDrive/rag_bio_project/data_pdfs, txt_dir=data_txt, urls=0\n","[INFO] loaded docs: 4\n","[INFO] Type-aware splitting started. total_docs=4, default_type=pdf\n","[INFO] Splitter ready for type='pdf' (size=1200, overlap=200)\n","[INFO] Split pdf source=/content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf -> 1 chunks (acc=1)\n","[INFO] Split pdf source=/content/drive/MyDrive/rag_bio_project/data_pdfs/wangmu.pdf -> 1 chunks (acc=2)\n","[INFO] Splitter ready for type='txt' (size=1000, overlap=150)\n","[INFO] Split txt source=data_txt/wangmu.txt -> 1 chunks (acc=3)\n","[INFO] Split txt source=data_txt/liqing.txt -> 1 chunks (acc=4)\n","[INFO] Type-aware splitting done. total_chunks=4\n","[INFO] chunks: 4\n","[INFO] Using HF embeddings: BAAI/bge-small-zh-v1.5 (normalize=True)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Computing embeddings for 4 chunks...\n","[INFO] Embedding dim=512\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Creating/updating collection: demo_user_LiQing\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: b73441563a04_0\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: bc51bb3cfc6d_1\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 70ef072143f5_2\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 84e5f75d65fe_3\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: b73441563a04_0\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: bc51bb3cfc6d_1\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 70ef072143f5_2\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 84e5f75d65fe_3\n"]},{"output_type":"error","ename":"InvalidDimensionException","evalue":"Embedding dimension 512 does not match collection dimensionality 1024","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidDimensionException\u001b[0m                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-580726524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# emb_cfg = EmbeddingConfig(provider='dashscope', model='text-embedding-v1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m created = build_embeddings_and_vectorstores(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharacter_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINDEX_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n","\u001b[0;32m/content/drive/MyDrive/rag_bio_project/src/embeddings.py\u001b[0m in \u001b[0;36mbuild_embeddings_and_vectorstores\u001b[0;34m(chunks, username, character_names, persist_dir, emb_cfg)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_vectors_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpersist_vectorstores_for_characters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"provider\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"normalize\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/rag_bio_project/src/embeddings.py\u001b[0m in \u001b[0;36mpersist_vectorstores_for_characters\u001b[0;34m(ids, texts, metas, vectors, persist_dir, username, character_names, emb_meta, collection_prefix)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mbatch_metas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0m_sanitize_meta_for_chroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mcoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] Collection '{coll_name}' upserted with {len(ids)} items.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mcreated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     def get(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_add\u001b[0;34m(self, ids, collection_id, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0muris\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muris\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         ):\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_embedding_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mrecords_to_submit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_producer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords_to_submit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_validate_embedding_record\u001b[0;34m(self, collection, record)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0madd_attributes_to_current_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"collection_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtrace_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SegmentAPI._validate_dimension\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenTelemetryGranularity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_validate_dimension\u001b[0;34m(self, collection, dim, update)\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collection_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dimension\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dimension\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             raise InvalidDimensionException(\n\u001b[0m\u001b[1;32m    819\u001b[0m                 \u001b[0;34mf\"Embedding dimension {dim} does not match collection dimensionality {collection['dimension']}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             )\n","\u001b[0;31mInvalidDimensionException\u001b[0m: Embedding dimension 512 does not match collection dimensionality 1024"]}],"source":["docs = load_sources(pdf_dir=str(DATA_PDFS), txt_dir=str(DATA_TXT), urls=None)\n","print('[INFO] loaded docs:', len(docs))\n","chunks = split_documents_type_aware(docs, default_type='pdf', verbose=True)\n","print('[INFO] chunks:', len(chunks))\n","\n","username = 'demo_user'\n","character_names = ['LiQing', 'WangMu']\n","\n","emb_cfg = EmbeddingConfig(provider='hf', model='BAAI/bge-small-zh-v1.5', normalize=True)\n","# Optional:\n","# emb_cfg = EmbeddingConfig(provider='openai', model='text-embedding-3-small')\n","# emb_cfg = EmbeddingConfig(provider='dashscope', model='text-embedding-v1')\n","\n","created = build_embeddings_and_vectorstores(\n","    chunks, username=username, character_names=character_names, persist_dir=str(INDEX_DIR), emb_cfg=emb_cfg\n",")\n","print('[INFO] Collections created:', created)"]},{"cell_type":"code","execution_count":null,"id":"yXFa31-dfMwa","metadata":{"id":"yXFa31-dfMwa","executionInfo":{"status":"aborted","timestamp":1759731894626,"user_tz":300,"elapsed":26486,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["from pathlib import Path\n","import textwrap, importlib, sys, json\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","SRC = PROJ/'src'\n","assert SRC.exists(), f\"src not found: {SRC}\"\n","\n","vf = SRC/'vectorstore.py'\n","vf.write_text(textwrap.dedent(r'''\n","from chromadb import PersistentClient\n","import json\n","\n","# We reuse the same embedder used for indexing\n","from embeddings import EmbeddingConfig, build_embeddings\n","\n","def _embedder_from_coll_meta(meta: dict):\n","    info = meta.get(\"embedding\")\n","    if isinstance(info, str):\n","        try:\n","            info = json.loads(info)\n","        except Exception:\n","            info = {}\n","    info = info or {}\n","    provider  = (info.get(\"provider\") or \"hf\")\n","    model     = (info.get(\"model\") or \"BAAI/bge-small-zh-v1.5\")\n","    normalize = bool(info.get(\"normalize\", True))\n","    return build_embeddings(EmbeddingConfig(provider=provider, model=model, normalize=normalize))\n","\n","def get_collection(persist_dir: str, collection_name: str):\n","    client = PersistentClient(path=persist_dir)\n","    return client.get_collection(collection_name)\n","\n","def quick_query(persist_dir: str, collection_name: str, query_text: str, n_results: int = 5):\n","    client = PersistentClient(path=persist_dir)\n","    coll = client.get_collection(collection_name)\n","    embedder = _embedder_from_coll_meta(coll.metadata or {})\n","    # Compute query embedding with the SAME model/dim as the collection\n","    if hasattr(embedder, \"embed_query\"):\n","        qvec = embedder.embed_query(query_text)\n","    else:\n","        qvec = embedder.embed_documents([query_text])[0]\n","    return coll.query(query_embeddings=[qvec], n_results=n_results,\n","                      include=[\"documents\",\"metadatas\",\"distances\"])\n","'''), encoding='utf-8')\n","\n","# 热重载\n","sys.path.insert(0, str(SRC))\n","import vectorstore\n","import importlib\n","importlib.reload(vectorstore)\n","from vectorstore import quick_query\n","print(\"✅ vectorstore.py patched & reloaded.\")\n"]},{"cell_type":"code","execution_count":null,"id":"EnS2XDXdaSrS","metadata":{"id":"EnS2XDXdaSrS","executionInfo":{"status":"aborted","timestamp":1759731894655,"user_tz":300,"elapsed":26513,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["print('\\n[TEST] Query LiQing collection:')\n","res = quick_query(str(INDEX_DIR), 'demo_user_LiQing', 'income and marriage', n_results=3)\n","for i,(doc,meta,dist) in enumerate(zip(res.get('documents',[['']])[0], res.get('metadatas',[['']])[0], res.get('distances',[['']])[0])):\n","    print('-'*60)\n","    print('rank', i+1, 'dist', dist)\n","    print('source', meta.get('source'))\n","    print(doc[:200].replace('\\n',' '))\n","\n","print('\\n[TEST] Query WangMu collection:')\n","res = quick_query(str(INDEX_DIR), 'demo_user_WangMu', 'risk control and marriage', n_results=3)\n","for i,(doc,meta,dist) in enumerate(zip(res.get('documents',[['']])[0], res.get('metadatas',[['']])[0], res.get('distances',[['']])[0])):\n","    print('-'*60)\n","    print('rank', i+1, 'dist', dist)\n","    print('source', meta.get('source'))\n","    print(doc[:200].replace('\\n',' '))"]},{"cell_type":"markdown","id":"7adf997c","metadata":{"id":"7adf997c"},"source":["# 🔧 RAG Pipeline — Retriever → Middleware → Prompt → LLM (Appended)\n","以下单元格基于你现有工程，补充完整的检索中间层与流水线测试，并可一键跑通 Demo。"]},{"cell_type":"code","execution_count":null,"id":"UrBSPRJujqaX","metadata":{"id":"UrBSPRJujqaX","executionInfo":{"status":"aborted","timestamp":1759731894709,"user_tz":300,"elapsed":26566,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["from chromadb import PersistentClient\n","client = PersistentClient(path=\"/content/drive/MyDrive/rag_bio_project/index\")\n","\n","for col in client.list_collections():\n","    coll = client.get_collection(col.name)\n","    print(\"name:\", coll.name, \"| id:\", coll.id, \"| count:\", coll.count())\n","    print(\"meta:\", coll.metadata)\n","    print(\"-\"*60)\n"]},{"cell_type":"code","source":["%pip -q install python-dotenv\n","\n","from pathlib import Path\n","from dotenv import load_dotenv, dotenv_values\n","import os\n","\n","PROJ = Path(\"/content/drive/MyDrive/rag_bio_project\")  # 按你的真实路径\n","ENV_PATH = PROJ / \".env\"\n","\n","# 读取看一下是否真的拿到 key\n","cfg = dotenv_values(ENV_PATH)      # 只读取，不写环境\n","print(\"[.env keys]\", list(cfg.keys()))\n","print(\"[OPENAI_API_KEY startswith sk?]\", str(cfg.get(\"OPENAI_API_KEY\",\"\"))[:7])\n","\n","# 真正写入当前进程的环境变量（override=True 覆盖已有值）\n","load_dotenv(ENV_PATH, override=True)\n","\n","print(\"[verify] OPENAI_API_KEY set:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"],"metadata":{"id":"n8NRBSSL29Az","executionInfo":{"status":"aborted","timestamp":1759731894710,"user_tz":300,"elapsed":26566,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"n8NRBSSL29Az","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"30838bc7","metadata":{"id":"30838bc7","executionInfo":{"status":"aborted","timestamp":1759731894722,"user_tz":300,"elapsed":26577,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# 0) 安装依赖（如已安装可跳过）\n","%pip -q install -U numpy==1.26.4 chromadb==0.4.24 langchain==0.2.11 langchain-core==0.2.26         langchain-community==0.2.10 langchain-openai==0.1.17 pypdf tiktoken\n","# %pip -q install -U langchain-ollama  # 如果要用本地 Ollama 模型\n","print(\"Deps OK\")\n"]},{"cell_type":"code","execution_count":null,"id":"12abb7f8","metadata":{"id":"12abb7f8","executionInfo":{"status":"aborted","timestamp":1759731894723,"user_tz":300,"elapsed":26577,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# 1) 路径设置：指向我们刚刚生成的工程目录\n","from pathlib import Path\n","PROJ = Path(\"/content/drive/MyDrive/rag_bio_project\")\n","SRC = PROJ/\"src\"\n","INDEX_DIR = PROJ/\"index\"     # 指向你的 Chroma 索引目录（Drive 中同名也可改这里）\n","import sys\n","sys.path.append(str(SRC))\n","print(\"Project:\", PROJ)\n","print(\"Index:\", INDEX_DIR)\n"]},{"cell_type":"code","execution_count":null,"id":"593ebeb0","metadata":{"id":"593ebeb0","executionInfo":{"status":"aborted","timestamp":1759731894748,"user_tz":300,"elapsed":26601,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# 2) 列出向量库 collections 以确认可见\n","from chromadb import PersistentClient\n","client = PersistentClient(path=str(INDEX_DIR))\n","cols = [c.name for c in client.list_collections()]\n","print(\"Collections:\", cols)\n"]},{"cell_type":"markdown","id":"d128a568","metadata":{"id":"d128a568"},"source":["## ✅ Retriever 冒烟测试（高阈值 + MMR + 分级）"]},{"cell_type":"code","source":["from pathlib import Path\n","from chromadb import PersistentClient\n","import json\n","\n","print(\"INDEX_DIR =\", INDEX_DIR)\n","client = PersistentClient(path=str(INDEX_DIR))\n","names = [c.name for c in client.list_collections()]\n","print(\"Collections:\", names)\n","\n","for n in names:\n","    coll = client.get_collection(n)\n","    meta = coll.metadata or {}\n","    emb = meta.get(\"embedding\")\n","    if isinstance(emb, str):\n","        try: emb = json.loads(emb)\n","        except: pass\n","    print(f\" - {n:<32} count={coll.count()}  embedding={emb}\")\n"],"metadata":{"id":"R_letEGl37Nk","executionInfo":{"status":"aborted","timestamp":1759731894764,"user_tz":300,"elapsed":26616,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"R_letEGl37Nk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from retriever import retrieve\n","\n","tmp = retrieve(\n","    persist_dir=str(INDEX_DIR),\n","    query_text=\"李青的年收入是多少？\",\n","    k=10, strategy=\"similarity\",\n","    fetch_k=50,\n","    score_threshold=0.0,     # 关闭阈值\n",")\n","\n","scores = [round(x[\"score\"], 4) for x in tmp.get(\"items\", [])]\n","print(\"raw scores (top 10):\", scores[:10])\n"],"metadata":{"id":"g7cTmMlX4S_E","executionInfo":{"status":"aborted","timestamp":1759731894766,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"g7cTmMlX4S_E","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from chromadb import PersistentClient\n","from retriever import _embedder_from_coll_meta  # 我们代码里已有\n","\n","client = PersistentClient(path=str(INDEX_DIR))\n","# 挑一个你确认有“李青”内容的集合名\n","cname = [n for n in names if \"LiQing\" in n or \"liqing\" in n.lower()][0]\n","coll = client.get_collection(cname)\n","\n","embedder = _embedder_from_coll_meta(coll.metadata or {})\n","q = \"李青 年 收入\"\n","qvec = (embedder.embed_query(q)\n","        if hasattr(embedder, \"embed_query\")\n","        else embedder.embed_documents([q])[0])\n","\n","qr = coll.query(query_embeddings=[qvec], n_results=5,\n","                include=[\"documents\",\"distances\",\"metadatas\"])\n","print(\"docs:\", qr[\"documents\"][0][:2])\n","print(\"dists:\", qr[\"distances\"][0][:2])\n"],"metadata":{"id":"UpU3dDMN4cc4","executionInfo":{"status":"aborted","timestamp":1759731894767,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"UpU3dDMN4cc4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"599dd73b","metadata":{"id":"599dd73b","executionInfo":{"status":"aborted","timestamp":1759731894768,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","from retriever import retrieve\n","question = \"李青的年收入是多少？\"\n","res = retrieve(persist_dir=str(INDEX_DIR), query_text=question,\n","               k=5, strategy=\"mmr\", strictness=\"strict\")\n","print(\"Route:\", res.get(\"route\"))\n","for it in res.get(\"items\", [])[:3]:\n","    print(it[\"grade\"], f\"{it['score']:.3f}\", \"src:\", (it[\"metadata\"] or {}).get(\"source\"))\n"]},{"cell_type":"markdown","id":"9b83ff81","metadata":{"id":"9b83ff81"},"source":["## 🧩 Middleware：角色识别 / 怀疑度（占位） / 人格（占位） / 复查（占位）"]},{"cell_type":"code","execution_count":null,"id":"5738f107","metadata":{"id":"5738f107","executionInfo":{"status":"aborted","timestamp":1759731894770,"user_tz":300,"elapsed":26618,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# from middleware import detect_characters_from_question, VerificationConfig, verify_answer_against_context\n","# det = detect_characters_from_question(\"请比较LiQing与WangMu的收入\", persist_dir=str(INDEX_DIR))\n","# print(\"Role detection:\", det)\n","# # 复查占位演示（默认不启用）\n","# vres = verify_answer_against_context(\"dummy answer\", res.get(\"items\", []), VerificationConfig(enabled=False))\n","# print(\"Verification (disabled):\", vres)\n"]},{"cell_type":"markdown","id":"934592ca","metadata":{"id":"934592ca"},"source":["## 🧠 Prompt 自动选择 + 预览"]},{"cell_type":"code","execution_count":null,"id":"025628fa","metadata":{"id":"025628fa","executionInfo":{"status":"aborted","timestamp":1759731894771,"user_tz":300,"elapsed":26618,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","from prompting import build_prompt_messages_auto\n","msgs, info = build_prompt_messages_auto(\"What is LiQing's annual income?\", res)\n","print(\"Mode:\", info[\"mode\"])\n","print(\"System msg preview:\", msgs[0].content[:160])\n"]},{"cell_type":"markdown","id":"e029bff5","metadata":{"id":"e029bff5"},"source":["## 🚀 一键流水线 Demo（需要 API Key）"]},{"cell_type":"code","execution_count":null,"id":"34c4db4b","metadata":{"id":"34c4db4b","executionInfo":{"status":"aborted","timestamp":1759731894771,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # 建议先用 medium，确认召回后再调高\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # 可选：在这里开启/调整中间层\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"李青的年收入是多少？\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"]},{"cell_type":"code","source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # 建议先用 medium，确认召回后再调高\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # 可选：在这里开启/调整中间层\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"李青, 你结婚了吗?以李青的口吻回答\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"],"metadata":{"id":"Gj7QbqGoJ5QN","executionInfo":{"status":"aborted","timestamp":1759731894772,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"Gj7QbqGoJ5QN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","print(\"OPENAI_API_KEY exists:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n","print(\"OPENAI_ORG:\", os.getenv(\"OPENAI_ORG\"))\n"],"metadata":{"id":"upaIbVgo-5lf","executionInfo":{"status":"aborted","timestamp":1759731894775,"user_tz":300,"elapsed":26619,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"upaIbVgo-5lf","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}