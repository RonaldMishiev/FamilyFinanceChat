{"cells":[{"cell_type":"markdown","id":"tEgHi1sbaSrQ","metadata":{"id":"tEgHi1sbaSrQ"},"source":["# üìò RAG Biography (Colab Edition)\n","\n","**One-click end-to-end:** install deps ‚Üí mount Drive ‚Üí set project path ‚Üí generate sample PDFs ‚Üí load ‚Üí split ‚Üí embed ‚Üí build Chroma collections `(username)_(character_name)` ‚Üí quick retrieval.\n","\n","> Default embeddings: **HuggingFace BGE small zh v1.5** (no API needed). You can switch to OpenAI/DashScope by editing the *Embeddings* cell.\n"]},{"cell_type":"code","execution_count":1,"id":"rXkdWBCNaSrR","metadata":{"id":"rXkdWBCNaSrR","executionInfo":{"status":"ok","timestamp":1759731868365,"user_tz":300,"elapsed":12,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["# # # ===== Clean + Pin =====\n","# import sys, subprocess, IPython\n","\n","# def sh(cmd):\n","#     print(\">>\", cmd)\n","#     subprocess.run(cmd, shell=True, check=False)\n","\n","# # 1) Ê∏Ö spacy ÂÆ∂ÊóèÔºà‰ºöÊãâ weasel/srsly/wasabiÔºåÂ∏∏ÊêÖÁâàÊú¨Ôºâ\n","# sh(\"pip -q uninstall -y spacy thinc catalogue srsly cymem preshed murmurhash wasabi blis typer langcodes || true\")\n","\n","# # 2) Âõ∫ÂÆö requestsÔºàÈÅøÂÖçÂíåÁ≥ªÁªüÂåÖÂÜ≤Á™ÅÔºâ\n","# sh(\"pip -q install -U requests==2.32.2\")\n","\n","# # 3) ÂÖ≥ÈîÆÔºöÂº∫Âà∂Êää numpy ÈôçÂà∞ 1.26.4ÔºàÂπ∂ÈÅøÂÖçÂÜçÊ¨°Ë¢´Âà´ÁöÑÂåÖÂçáÂõûÂéªÔºâ\n","# sh('pip -q install --force-reinstall --no-build-isolation \"numpy==1.26.4\"')\n","\n","# # 4) ‰Ω†ÁöÑ‰æùËµñ\n","# sh(\"pip -q install -U chromadb==0.4.24 \"\n","#    \"langchain==0.2.11 langchain-core==0.2.26 langchain-community==0.2.10 \"\n","#    \"langchain-openai==0.1.17 pypdf tiktoken\")\n","\n","# # 5) Ê†°È™å‰æùËµñÂÜ≤Á™ÅÔºàÂèØÈÄâÔºâ\n","# sh(\"pip -q check || true\")\n","\n","# sh(\"pip -q install -U reportlab python-dotenv\")\n","\n","# print(\"\\n‚úÖ ÂÆâË£ÖÂÆåÊàêÔºöÂç≥Â∞ÜÈáçÂêØÂÜÖÊ†∏‰ª•ÂàáÊç¢Âà∞ NumPy 1.26.4 ...\")\n","# IPython.get_ipython().kernel.do_shutdown(restart=True)  # Ëá™Âä®ÈáçÂêØ\n"]},{"cell_type":"code","execution_count":2,"id":"v6qY0wVfaSrR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6qY0wVfaSrR","outputId":"11ca3d8b-bb79-49f3-a9bc-67498c09ce23","executionInfo":{"status":"ok","timestamp":1759731868957,"user_tz":300,"elapsed":578,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Project path: /content/drive/MyDrive/rag_bio_project\n","‚úÖ Folders ready.\n"]}],"source":["from google.colab import drive\n","from pathlib import Path\n","drive.mount('/content/drive')\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","print('Project path:', PROJ)\n","\n","for d in [PROJ, PROJ/'src', PROJ/'data_pdfs', PROJ/'data_txt', PROJ/'index', PROJ/'notebooks']:\n","    d.mkdir(parents=True, exist_ok=True)\n","print('‚úÖ Folders ready.')"]},{"cell_type":"code","execution_count":3,"id":"VUOy578IaSrR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUOy578IaSrR","outputId":"0caa3822-7a16-4206-d68a-3d382a3165e8","executionInfo":{"status":"ok","timestamp":1759731868972,"user_tz":300,"elapsed":14,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[SKIP ] /content/drive/MyDrive/rag_bio_project/src/loader.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/splitter.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/embeddings.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/vectorstore.py\n","‚úÖ src/ files ensured.\n"]}],"source":["from pathlib import Path\n","import textwrap\n","\n","SRC = PROJ/'src'\n","\n","def ensure_file(path: Path, content: str):\n","    if not path.exists():\n","        path.write_text(textwrap.dedent(content), encoding='utf-8')\n","        print('[WRITE]', path)\n","    else:\n","        print('[SKIP ]', path)\n","\n","# --- loader.py ---\n","ensure_file(SRC/'loader.py', '''\n","from pathlib import Path\n","from typing import List, Optional, Iterable, Dict\n","import hashlib, re, time\n","from urllib.parse import urlsplit, urlunsplit\n","from langchain_core.documents import Document\n","from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n","def _normalize_text(s: str) -> str:\n","    if not s: return ''\n","    s = s.replace('\\ufeff', '').replace('\\xa0', ' ')\n","    s = re.sub(r'[ \\t]+', ' ', s)\n","    s = re.sub(r'\\n{3,}', '\\n\\n', s)\n","    return s.strip()\n","def _clean_url(u: str) -> str:\n","    u = u.strip().replace(' ', '')\n","    parts = list(urlsplit(u))\n","    if not parts[0]: parts[0] = 'https'\n","    return urlunsplit(parts)\n","def _doc_hash(content: str) -> str:\n","    return hashlib.md5(content.encode('utf-8', errors='ignore')).hexdigest()[:16]\n","def _filter_content(text: str, min_chars: int, max_chars: Optional[int]) -> bool:\n","    n = len(text)\n","    if n < min_chars: return False\n","    if max_chars is not None and n > max_chars: return False\n","    return True\n","def load_sources(pdf_dir: str='data_pdfs', txt_dir: Optional[str]='data_txt', urls: Optional[List[str]] = None,\n","                 recursive: bool=True, txt_extensions: Iterable[str]=( '.txt', '.md'), txt_encoding: str='utf-8',\n","                 pages: Optional[str]=None, min_chars: int=50, max_chars: Optional[int]=None,\n","                 headers: Optional[Dict[str,str]] = None, timeout: int=15, max_retries: int=2) -> List[Document]:\n","    docs: List[Document] = []\n","    total_raw = 0\n","    pdir = Path(pdf_dir)\n","    if pdir.exists():\n","        for p in (pdir.rglob('*.pdf') if recursive else pdir.glob('*.pdf')):\n","            try:\n","                loader = PyPDFLoader(str(p))\n","                loaded = loader.load()\n","                if pages:\n","                    parts = [None if x in ('', 'None', None) else int(x) for x in pages.split(':')]\n","                    start = parts[0] if len(parts)>0 else None\n","                    stop  = parts[1] if len(parts)>1 else None\n","                    step  = parts[2] if len(parts)>2 else None\n","                    loaded = [d for d in loaded[slice(start, stop, step)]]\n","                for d in loaded:\n","                    d.page_content = _normalize_text(d.page_content)\n","                    d.metadata['source'] = str(p)\n","                    d.metadata['source_type'] = 'pdf'\n","                    d.metadata['loader_info'] = {'type':'PyPDFLoader','pages':pages}\n","                    if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                total_raw += len(loaded)\n","            except Exception as e:\n","                print(f'[WARN] Skip PDF {p}: {e}')\n","    if txt_dir:\n","        tdir = Path(txt_dir)\n","        if tdir.exists():\n","            exts = {e.lower() for e in txt_extensions}\n","            for p in (tdir.rglob('*') if recursive else tdir.glob('*')):\n","                if p.is_file() and p.suffix.lower() in exts:\n","                    try:\n","                        loader = TextLoader(str(p), encoding=txt_encoding)\n","                        loaded = loader.load()\n","                        for d in loaded:\n","                            d.page_content = _normalize_text(d.page_content)\n","                            d.metadata['source'] = str(p)\n","                            d.metadata['source_type'] = 'txt'\n","                            d.metadata['loader_info'] = {'type':'TextLoader','encoding':txt_encoding}\n","                            if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                        total_raw += len(loaded)\n","                    except Exception as e:\n","                        print(f'[WARN] Skip TXT {p}: {e}')\n","    if urls:\n","        urls = [u for u in (urls or []) if isinstance(u, str) and u.strip()]\n","        for u in urls:\n","            url = _clean_url(u)\n","            tries = 0\n","            while True:\n","                try:\n","                    loader = WebBaseLoader(url, requests_kwargs={'headers': headers or {'User-Agent':'Mozilla/5.0 (RAG-Loader/1.0)'}, 'timeout': timeout})\n","                    loaded = loader.load()\n","                    for d in loaded:\n","                        d.page_content = _normalize_text(d.page_content)\n","                        d.metadata['source'] = url\n","                        d.metadata['source_type'] = 'web'\n","                        d.metadata['loader_info'] = {'type':'WebBaseLoader','timeout':timeout,'headers':bool(headers)}\n","                        if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                    total_raw += len(loaded)\n","                    break\n","                except Exception as e:\n","                    tries += 1\n","                    if tries > max_retries:\n","                        print(f'[WARN] Skip URL {url} after {max_retries} retries: {e}'); break\n","                    print(f'[INFO] Retry {tries}/{max_retries} for URL {url} due to: {e}')\n","    seen = set(); uniq: List[Document] = []\n","    for d in docs:\n","        h = _doc_hash(d.page_content)\n","        key = (d.metadata.get('source'), h)\n","        if key not in seen:\n","            seen.add(key); uniq.append(d)\n","    print(f'[INFO] Loader complete. raw={total_raw}, kept={len(uniq)}')\n","    return uniq\n","''')\n","\n","# --- splitter.py ---\n","ensure_file(SRC/'splitter.py', '''\n","from typing import List, Dict\n","from dataclasses import dataclass\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","@dataclass\n","class SplitterProfile:\n","    chunk_size: int; chunk_overlap: int; separators: List[str]\n","PDF_PROFILE = SplitterProfile(1200, 200, ['\\\\n\\\\n','\\\\n','„ÄÇ','ÔºÅ','Ôºü','.', '!', '?', ' ', ''])\n","TXT_PROFILE = SplitterProfile(1000, 150, ['\\\\n\\\\n','\\\\n','.', '?', '!', '„ÄÇ','Ôºü','ÔºÅ',' ', ''])\n","WEB_PROFILE = SplitterProfile(900, 150,  ['\\\\n\\\\n','\\\\n','„ÄÇ','ÔºÅ','Ôºü','.', '!', '?', ' ', ''])\n","PROFILE_MAP: Dict[str, SplitterProfile] = {'pdf':PDF_PROFILE,'txt':TXT_PROFILE,'web':WEB_PROFILE}\n","def _build_splitter(p: SplitterProfile) -> RecursiveCharacterTextSplitter:\n","    overlap = p.chunk_overlap if p.chunk_overlap < p.chunk_size else max(0, min(p.chunk_size//5, 200))\n","    if overlap != p.chunk_overlap: print(f'[WARN] overlap >= size; fallback to {overlap}')\n","    return RecursiveCharacterTextSplitter(chunk_size=p.chunk_size, chunk_overlap=overlap, separators=p.separators)\n","def split_documents_type_aware(docs: List[Document], default_type: str='pdf', verbose: bool=True) -> List[Document]:\n","    if verbose: print(f'[INFO] Type-aware splitting started. total_docs={len(docs)}, default_type={default_type}')\n","    splitter_cache: Dict[str, RecursiveCharacterTextSplitter] = {}\n","    def get_splitter(kind: str):\n","        typ = (kind or '').lower(); typ = typ if typ in PROFILE_MAP else default_type\n","        if typ not in splitter_cache:\n","            splitter_cache[typ] = _build_splitter(PROFILE_MAP[typ])\n","            if verbose:\n","                p = PROFILE_MAP[typ]; print(f\"[INFO] Splitter ready for type='{typ}' (size={p.chunk_size}, overlap={p.chunk_overlap})\")\n","        return splitter_cache[typ]\n","    out: List[Document] = []; per_source_index: Dict[str,int] = {}\n","    for d in docs:\n","        stype = (d.metadata.get('source_type') or default_type).lower()\n","        splitter = get_splitter(stype)\n","        chunks = splitter.split_documents([d])\n","        source_key = str(d.metadata.get('source', 'unknown'))\n","        start_idx = per_source_index.get(source_key, 0)\n","        for i, c in enumerate(chunks):\n","            c.metadata = dict(d.metadata) | {'chunk_index': start_idx + i, 'splitter_profile': stype}\n","            out.append(c)\n","        per_source_index[source_key] = start_idx + len(chunks)\n","        if verbose: print(f\"[INFO] Split {stype} source={source_key} -> {len(chunks)} chunks (acc={len(out)})\")\n","    if verbose: print(f'[INFO] Type-aware splitting done. total_chunks={len(out)}')\n","    return out\n","''')\n","\n","# --- embeddings.py ---\n","ensure_file(SRC/'embeddings.py', '''\n","import os, re, time, hashlib\n","from dataclasses import dataclass\n","from typing import List, Tuple, Dict, Optional\n","from langchain_core.documents import Document\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.embeddings import DashScopeEmbeddings\n","from chromadb import PersistentClient\n","@dataclass\n","class EmbeddingConfig:\n","    provider: str = 'hf'\n","    model: str = 'BAAI/bge-small-zh-v1.5'\n","    normalize: bool = True\n","def _sanitize_name(s: str, fallback_prefix: str='user') -> str:\n","    s2 = re.sub(r'[^a-zA-Z0-9_]+', '_', str(s or '')).strip('_')\n","    return s2 or f\"{fallback_prefix}_{int(time.time())}\"\n","def _doc_id(meta: Dict, idx: int) -> str:\n","    base = f\"{meta.get('source','unknown')}|{meta.get('chunk_index', idx)}\"\n","    return hashlib.md5(base.encode('utf-8', errors='ignore')).hexdigest()[:12] + f'_{idx}'\n","def build_embeddings(cfg: EmbeddingConfig):\n","    prov = cfg.provider.lower()\n","    if prov == 'hf':\n","        print(f\"[INFO] Using HF embeddings: {cfg.model} (normalize={cfg.normalize})\")\n","        return HuggingFaceBgeEmbeddings(model_name=cfg.model, encode_kwargs={'normalize_embeddings': cfg.normalize})\n","    elif prov == 'openai':\n","        if not os.getenv('OPENAI_API_KEY'): raise RuntimeError('OPENAI_API_KEY is not set.')\n","        model_name = cfg.model or 'text-embedding-3-small'\n","        print(f\"[INFO] Using OpenAI embeddings: {model_name}\")\n","        return OpenAIEmbeddings(model=model_name)\n","    elif prov == 'dashscope':\n","        if not os.getenv('DASHSCOPE_API_KEY'): raise RuntimeError('DASHSCOPE_API_KEY is not set.')\n","        model_name = cfg.model or 'text-embedding-v1'\n","        print(f\"[INFO] Using DashScope embeddings: {model_name}\")\n","        return DashScopeEmbeddings(model=model_name)\n","    else:\n","        raise ValueError(f'Unknown provider: {cfg.provider}')\n","def compute_vectors_once(chunks: List[Document], embedder):\n","    texts = [c.page_content for c in chunks]\n","    metas = [dict(c.metadata or {}) for c in chunks]\n","    ids = [_doc_id(m, i) for i, m in enumerate(metas)]\n","    print(f\"[INFO] Computing embeddings for {len(texts)} chunks...\")\n","    vectors = embedder.embed_documents(texts)\n","    if not vectors or not vectors[0]: raise RuntimeError('Empty embeddings returned.')\n","    dim = len(vectors[0])\n","    emb_info = {'provider': type(embedder).__name__, 'dim': dim}\n","    print(f\"[INFO] Embedding dim={dim}\")\n","    return ids, texts, metas, vectors, emb_info\n","def persist_vectorstores_for_characters(ids, texts, metas, vectors, persist_dir, username, character_names, emb_meta, collection_prefix: Optional[str]=None):\n","    user_tag = _sanitize_name(username, 'user')\n","    created = []\n","    client: PersistentClient = PersistentClient(path=persist_dir)\n","    for cname in character_names:\n","        char_tag = _sanitize_name(cname, 'char')\n","        base_name = f\"{user_tag}_{char_tag}\"\n","        coll_name = base_name if not collection_prefix else _sanitize_name(f\"{collection_prefix}_{base_name}\", 'coll')\n","        print(f\"[INFO] Creating/updating collection: {coll_name}\")\n","        coll = client.get_or_create_collection(name=coll_name, metadata={'embedding': emb_meta})\n","        B = 256\n","        for i in range(0, len(ids), B):\n","            coll.add(ids=ids[i:i+B], documents=texts[i:i+B], metadatas=metas[i:i+B], embeddings=vectors[i:i+B])\n","        print(f\"[INFO] Collection '{coll_name}' upserted with {len(ids)} items.\")\n","        created.append(coll_name)\n","    print(f\"[INFO] Done. Created/updated {len(created)} collections.\")\n","    return created\n","def build_embeddings_and_vectorstores(chunks: List[Document], username: str, character_names: List[str], persist_dir: str='index', emb_cfg: EmbeddingConfig = EmbeddingConfig()):\n","    if not character_names: raise ValueError('character_names must not be empty.')\n","    embedder = build_embeddings(emb_cfg)\n","    ids, texts, metas, vectors, emb_info = compute_vectors_once(chunks, embedder)\n","    return persist_vectorstores_for_characters(ids, texts, metas, vectors, persist_dir, username, character_names, {'provider':emb_cfg.provider,'model':emb_cfg.model,'dim':emb_info['dim'],'normalize':emb_cfg.normalize})\n","''')\n","\n","# --- vectorstore.py ---\n","ensure_file(SRC/'vectorstore.py', '''\n","from chromadb import PersistentClient\n","def get_collection(persist_dir: str, collection_name: str):\n","    client = PersistentClient(path=persist_dir)\n","    return client.get_collection(collection_name)\n","def quick_query(persist_dir: str, collection_name: str, query_text: str, n_results: int=5):\n","    coll = get_collection(persist_dir, collection_name)\n","    return coll.query(query_texts=[query_text], n_results=n_results, include=['documents','metadatas','distances'])\n","''')\n","\n","print('‚úÖ src/ files ensured.')"]},{"cell_type":"code","execution_count":4,"id":"bqtMUD1CaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqtMUD1CaSrS","outputId":"c16a1aa9-bd6a-48be-98fd-b15f867cbab2","executionInfo":{"status":"ok","timestamp":1759731875074,"user_tz":300,"elapsed":6097,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Added to sys.path: /content/drive/MyDrive/rag_bio_project/src\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Imports OK\n"]}],"source":["import sys\n","from pathlib import Path\n","CANDIDATES = [PROJ, Path.cwd(), Path('/content/drive/MyDrive/rag_bio_project')]\n","project_src=None\n","for base in CANDIDATES:\n","    src = base/'src'\n","    if src.exists() and (src/'loader.py').exists():\n","        project_src=src.resolve(); break\n","assert project_src, 'src/ not found. Check PROJ path.'\n","sys.path.insert(0, str(project_src))\n","print('[INFO] Added to sys.path:', project_src)\n","\n","from loader import load_sources\n","from splitter import split_documents_type_aware\n","from embeddings import EmbeddingConfig, build_embeddings_and_vectorstores\n","from vectorstore import quick_query\n","print('‚úÖ Imports OK')"]},{"cell_type":"code","execution_count":5,"id":"B9t-jeBCaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9t-jeBCaSrS","outputId":"e8e449de-b225-4bd6-aa2a-9c9aefdfa552","executionInfo":{"status":"ok","timestamp":1759731876751,"user_tz":300,"elapsed":1665,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] ENV loaded. OPENAI_API_KEY exists: False\n","[INFO] PDFs generated at /content/drive/MyDrive/rag_bio_project/data_pdfs\n","[INFO] TXT samples ensured at data_txt\n"]}],"source":["from reportlab.lib.pagesizes import letter\n","from reportlab.pdfgen import canvas\n","from dotenv import load_dotenv\n","import os\n","\n","DATA_PDFS = PROJ/'data_pdfs'\n","DATA_TXT  = PROJ/'data_txt'\n","INDEX_DIR = PROJ/'index'\n","ENV_PATH  = PROJ/' .env'\n","load_dotenv(ENV_PATH)\n","print('[INFO] ENV loaded. OPENAI_API_KEY exists:', bool(os.getenv('OPENAI_API_KEY')))\n","\n","def make_pdf(path, title, lines):\n","    c = canvas.Canvas(str(path), pagesize=letter)\n","    width, height = letter\n","    y = height - 72\n","    c.setFont('Times-Roman', 12)\n","    c.drawString(72, y, title); y -= 24\n","    for ln in lines:\n","        for seg in ln.split('\\n'):\n","            c.drawString(72, y, seg); y -= 18\n","            if y < 72:\n","                c.showPage(); y = height - 72; c.setFont('Times-Roman', 12)\n","    c.save()\n","\n","make_pdf(DATA_PDFS/'liqing.pdf', 'Li Qing Biography', [\n","    'Born in Suzhou in 1990; studied Economics and became a product manager.',\n","    'Finance: ~350k CNY income; index funds; medium risk preference.',\n","    'Marriage: married in 2018; one daughter in 2022.',\n","    'Experience: 2020-2022 SEA market localization projects.'\n","])\n","make_pdf(DATA_PDFS/'wangmu.pdf', 'Wang Mu Biography', [\n","    'Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations.',\n","    'Finance: ~200k CNY income; house + money-market funds; conservative.',\n","    'Marriage: single; passionate about education equity.',\n","    'Experience: 2015-2019 broker risk control; compliance expertise.'\n","])\n","print('[INFO] PDFs generated at', DATA_PDFS)\n","\n","from pathlib import Path\n","\n","DATA_TXT = Path(\"data_txt\")\n","DATA_TXT.mkdir(parents=True, exist_ok=True)\n","\n","liqing = \"\\n\".join([\n","    \"Born in Suzhou in 1990; studied Economics and became a product manager.\",\n","    \"Finance: ~350k CNY income; index funds; medium risk preference.\",\n","    \"Marriage: married in 2018; one daughter in 2022.\",\n","    \"Experience: 2020-2022 SEA market localization projects.\",\n","])\n","(DATA_TXT / \"liqing.txt\").write_text(liqing, encoding=\"utf-8\")\n","\n","wangmu = \"\\n\".join([\n","    \"Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations.\",\n","    \"Finance: ~200k CNY income; house + money-market funds; conservative.\",\n","    \"Marriage: single; passionate about education equity.\",\n","    \"Experience: 2015-2019 broker risk control; compliance expertise.\",\n","])\n","(DATA_TXT / \"wangmu.txt\").write_text(wangmu, encoding=\"utf-8\")\n","\n","print('[INFO] TXT samples ensured at', DATA_TXT)"]},{"cell_type":"code","execution_count":6,"id":"edEvIZTUeyMj","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edEvIZTUeyMj","outputId":"75e966d8-1590-4fda-c36b-fe43dbbc8aca","executionInfo":{"status":"ok","timestamp":1759731876778,"user_tz":300,"elapsed":28,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úî loader.py patched\n","‚úî embeddings.py patched\n","‚úÖ Patch applied & modules reloaded.\n"]}],"source":["from pathlib import Path\n","import importlib, re, json, sys\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","SRC = PROJ/'src'\n","assert SRC.exists(), f\"src not found: {SRC}\"\n","\n","# 1) patch loader.py: make loader_info a JSON string (not a dict)\n","lp = SRC/'loader.py'\n","ls = lp.read_text(encoding='utf-8')\n","\n","if 'import json' not in ls:\n","    ls = ls.replace('from urllib.parse import urlsplit, urlunsplit',\n","                    'from urllib.parse import urlsplit, urlunsplit\\nimport json')\n","\n","# PyPDFLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'PyPDFLoader','pages':pages}, ensure_ascii=False)\",\n","    ls\n",")\n","# TextLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'TextLoader','encoding':txt_encoding}, ensure_ascii=False)\",\n","    ls,\n","    count=1  # only replace the TXT occurrence once\n",")\n","# WebBaseLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'WebBaseLoader','timeout':timeout,'headers':bool(headers)}, ensure_ascii=False)\",\n","    ls,\n","    count=1  # only replace the Web occurrence once\n",")\n","\n","lp.write_text(ls, encoding='utf-8')\n","print(\"‚úî loader.py patched\")\n","\n","# 2) patch embeddings.py: (a) collection metadata -> JSON string (b) sanitize per-doc metadatas before add()\n","ep = SRC/'embeddings.py'\n","es = ep.read_text(encoding='utf-8')\n","\n","if 'import json' not in es:\n","    es = es.replace('from chromadb import PersistentClient',\n","                    'from chromadb import PersistentClient\\nimport json')\n","\n","# add a sanitizer\n","if '_sanitize_meta_for_chroma' not in es:\n","    es = es.replace(\n","        'def compute_vectors_once(chunks: List[Document], embedder):',\n","        \"\"\"def _sanitize_meta_for_chroma(m: dict) -> dict:\n","    out = {}\n","    for k, v in (m or {}).items():\n","        if isinstance(v, (str, int, float, bool)):\n","            out[k] = v\n","        else:\n","            out[k] = json.dumps(v, ensure_ascii=False)\n","    return out\n","\n","def compute_vectors_once(chunks: List[Document], embedder):\"\"\"\n","    )\n","\n","# ensure collection metadata uses JSON string\n","es = es.replace(\n","    \"metadata={'embedding': emb_meta}\",\n","    \"metadata={'embedding': json.dumps(emb_meta, ensure_ascii=False)}\"\n",")\n","\n","# sanitize metas list before coll.add\n","es = re.sub(\n","    r\"for i in range\\(0, len\\(ids\\), B\\):\\s*\"\n","    r\"coll\\.add\\(ids=ids\\[i:i\\+B\\], documents=texts\\[i:i\\+B\\], metadatas=metas\\[i:i\\+B\\], embeddings=vectors\\[i:i\\+B\\]\\)\",\n","    \"for i in range(0, len(ids), B):\\n\"\n","    \"            batch_metas = [ _sanitize_meta_for_chroma(m) for m in metas[i:i+B] ]\\n\"\n","    \"            coll.add(ids=ids[i:i+B], documents=texts[i:i+B], metadatas=batch_metas, embeddings=vectors[i:i+B])\",\n","    es\n",")\n","\n","ep.write_text(es, encoding='utf-8')\n","print(\"‚úî embeddings.py patched\")\n","\n","# 3) reload modules\n","sys.path.insert(0, str(SRC))\n","import loader, embeddings\n","import importlib\n","importlib.reload(loader)\n","importlib.reload(embeddings)\n","from embeddings import EmbeddingConfig, build_embeddings_and_vectorstores\n","print(\"‚úÖ Patch applied & modules reloaded.\")\n"]},{"cell_type":"code","execution_count":7,"id":"aEENRsoyaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":967},"id":"aEENRsoyaSrS","outputId":"d6793154-1c7e-4b1c-b14e-a8f15b329c5d","executionInfo":{"status":"error","timestamp":1759731894658,"user_tz":300,"elapsed":17878,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Loader complete. raw=4, kept=4, pdf_dir=/content/drive/MyDrive/rag_bio_project/data_pdfs, txt_dir=data_txt, urls=0\n","[INFO] loaded docs: 4\n","[INFO] Type-aware splitting started. total_docs=4, default_type=pdf\n","[INFO] Splitter ready for type='pdf' (size=1200, overlap=200)\n","[INFO] Split pdf source=/content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf -> 1 chunks (acc=1)\n","[INFO] Split pdf source=/content/drive/MyDrive/rag_bio_project/data_pdfs/wangmu.pdf -> 1 chunks (acc=2)\n","[INFO] Splitter ready for type='txt' (size=1000, overlap=150)\n","[INFO] Split txt source=data_txt/wangmu.txt -> 1 chunks (acc=3)\n","[INFO] Split txt source=data_txt/liqing.txt -> 1 chunks (acc=4)\n","[INFO] Type-aware splitting done. total_chunks=4\n","[INFO] chunks: 4\n","[INFO] Using HF embeddings: BAAI/bge-small-zh-v1.5 (normalize=True)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Computing embeddings for 4 chunks...\n","[INFO] Embedding dim=512\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Creating/updating collection: demo_user_LiQing\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: b73441563a04_0\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: bc51bb3cfc6d_1\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 70ef072143f5_2\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 84e5f75d65fe_3\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: b73441563a04_0\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: bc51bb3cfc6d_1\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 70ef072143f5_2\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Add of existing embedding ID: 84e5f75d65fe_3\n"]},{"output_type":"error","ename":"InvalidDimensionException","evalue":"Embedding dimension 512 does not match collection dimensionality 1024","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidDimensionException\u001b[0m                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-580726524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# emb_cfg = EmbeddingConfig(provider='dashscope', model='text-embedding-v1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m created = build_embeddings_and_vectorstores(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharacter_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINDEX_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n","\u001b[0;32m/content/drive/MyDrive/rag_bio_project/src/embeddings.py\u001b[0m in \u001b[0;36mbuild_embeddings_and_vectorstores\u001b[0;34m(chunks, username, character_names, persist_dir, emb_cfg)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_vectors_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpersist_vectorstores_for_characters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersist_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"provider\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"normalize\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memb_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/rag_bio_project/src/embeddings.py\u001b[0m in \u001b[0;36mpersist_vectorstores_for_characters\u001b[0;34m(ids, texts, metas, vectors, persist_dir, username, character_names, emb_meta, collection_prefix)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mbatch_metas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0m_sanitize_meta_for_chroma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mcoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[INFO] Collection '{coll_name}' upserted with {len(ids)} items.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mcreated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     def get(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_add\u001b[0;34m(self, ids, collection_id, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0muris\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muris\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         ):\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_embedding_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0mrecords_to_submit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_producer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords_to_submit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_validate_embedding_record\u001b[0;34m(self, collection, record)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0madd_attributes_to_current_span\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"collection_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtrace_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SegmentAPI._validate_dimension\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenTelemetryGranularity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/telemetry/opentelemetry/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrace_granularity\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgranularity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/segment.py\u001b[0m in \u001b[0;36m_validate_dimension\u001b[0;34m(self, collection, dim, update)\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collection_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dimension\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dimension\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             raise InvalidDimensionException(\n\u001b[0m\u001b[1;32m    819\u001b[0m                 \u001b[0;34mf\"Embedding dimension {dim} does not match collection dimensionality {collection['dimension']}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             )\n","\u001b[0;31mInvalidDimensionException\u001b[0m: Embedding dimension 512 does not match collection dimensionality 1024"]}],"source":["docs = load_sources(pdf_dir=str(DATA_PDFS), txt_dir=str(DATA_TXT), urls=None)\n","print('[INFO] loaded docs:', len(docs))\n","chunks = split_documents_type_aware(docs, default_type='pdf', verbose=True)\n","print('[INFO] chunks:', len(chunks))\n","\n","username = 'demo_user'\n","character_names = ['LiQing', 'WangMu']\n","\n","emb_cfg = EmbeddingConfig(provider='hf', model='BAAI/bge-small-zh-v1.5', normalize=True)\n","# Optional:\n","# emb_cfg = EmbeddingConfig(provider='openai', model='text-embedding-3-small')\n","# emb_cfg = EmbeddingConfig(provider='dashscope', model='text-embedding-v1')\n","\n","created = build_embeddings_and_vectorstores(\n","    chunks, username=username, character_names=character_names, persist_dir=str(INDEX_DIR), emb_cfg=emb_cfg\n",")\n","print('[INFO] Collections created:', created)"]},{"cell_type":"code","execution_count":null,"id":"yXFa31-dfMwa","metadata":{"id":"yXFa31-dfMwa","executionInfo":{"status":"aborted","timestamp":1759731894626,"user_tz":300,"elapsed":26486,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["from pathlib import Path\n","import textwrap, importlib, sys, json\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","SRC = PROJ/'src'\n","assert SRC.exists(), f\"src not found: {SRC}\"\n","\n","vf = SRC/'vectorstore.py'\n","vf.write_text(textwrap.dedent(r'''\n","from chromadb import PersistentClient\n","import json\n","\n","# We reuse the same embedder used for indexing\n","from embeddings import EmbeddingConfig, build_embeddings\n","\n","def _embedder_from_coll_meta(meta: dict):\n","    info = meta.get(\"embedding\")\n","    if isinstance(info, str):\n","        try:\n","            info = json.loads(info)\n","        except Exception:\n","            info = {}\n","    info = info or {}\n","    provider  = (info.get(\"provider\") or \"hf\")\n","    model     = (info.get(\"model\") or \"BAAI/bge-small-zh-v1.5\")\n","    normalize = bool(info.get(\"normalize\", True))\n","    return build_embeddings(EmbeddingConfig(provider=provider, model=model, normalize=normalize))\n","\n","def get_collection(persist_dir: str, collection_name: str):\n","    client = PersistentClient(path=persist_dir)\n","    return client.get_collection(collection_name)\n","\n","def quick_query(persist_dir: str, collection_name: str, query_text: str, n_results: int = 5):\n","    client = PersistentClient(path=persist_dir)\n","    coll = client.get_collection(collection_name)\n","    embedder = _embedder_from_coll_meta(coll.metadata or {})\n","    # Compute query embedding with the SAME model/dim as the collection\n","    if hasattr(embedder, \"embed_query\"):\n","        qvec = embedder.embed_query(query_text)\n","    else:\n","        qvec = embedder.embed_documents([query_text])[0]\n","    return coll.query(query_embeddings=[qvec], n_results=n_results,\n","                      include=[\"documents\",\"metadatas\",\"distances\"])\n","'''), encoding='utf-8')\n","\n","# ÁÉ≠ÈáçËΩΩ\n","sys.path.insert(0, str(SRC))\n","import vectorstore\n","import importlib\n","importlib.reload(vectorstore)\n","from vectorstore import quick_query\n","print(\"‚úÖ vectorstore.py patched & reloaded.\")\n"]},{"cell_type":"code","execution_count":null,"id":"EnS2XDXdaSrS","metadata":{"id":"EnS2XDXdaSrS","executionInfo":{"status":"aborted","timestamp":1759731894655,"user_tz":300,"elapsed":26513,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["print('\\n[TEST] Query LiQing collection:')\n","res = quick_query(str(INDEX_DIR), 'demo_user_LiQing', 'income and marriage', n_results=3)\n","for i,(doc,meta,dist) in enumerate(zip(res.get('documents',[['']])[0], res.get('metadatas',[['']])[0], res.get('distances',[['']])[0])):\n","    print('-'*60)\n","    print('rank', i+1, 'dist', dist)\n","    print('source', meta.get('source'))\n","    print(doc[:200].replace('\\n',' '))\n","\n","print('\\n[TEST] Query WangMu collection:')\n","res = quick_query(str(INDEX_DIR), 'demo_user_WangMu', 'risk control and marriage', n_results=3)\n","for i,(doc,meta,dist) in enumerate(zip(res.get('documents',[['']])[0], res.get('metadatas',[['']])[0], res.get('distances',[['']])[0])):\n","    print('-'*60)\n","    print('rank', i+1, 'dist', dist)\n","    print('source', meta.get('source'))\n","    print(doc[:200].replace('\\n',' '))"]},{"cell_type":"markdown","id":"7adf997c","metadata":{"id":"7adf997c"},"source":["# üîß RAG Pipeline ‚Äî Retriever ‚Üí Middleware ‚Üí Prompt ‚Üí LLM (Appended)\n","‰ª•‰∏ãÂçïÂÖÉÊ†ºÂü∫‰∫é‰Ω†Áé∞ÊúâÂ∑•Á®ãÔºåË°•ÂÖÖÂÆåÊï¥ÁöÑÊ£ÄÁ¥¢‰∏≠Èó¥Â±Ç‰∏éÊµÅÊ∞¥Á∫øÊµãËØïÔºåÂπ∂ÂèØ‰∏ÄÈîÆË∑ëÈÄö Demo„ÄÇ"]},{"cell_type":"code","execution_count":null,"id":"UrBSPRJujqaX","metadata":{"id":"UrBSPRJujqaX","executionInfo":{"status":"aborted","timestamp":1759731894709,"user_tz":300,"elapsed":26566,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["from chromadb import PersistentClient\n","client = PersistentClient(path=\"/content/drive/MyDrive/rag_bio_project/index\")\n","\n","for col in client.list_collections():\n","    coll = client.get_collection(col.name)\n","    print(\"name:\", coll.name, \"| id:\", coll.id, \"| count:\", coll.count())\n","    print(\"meta:\", coll.metadata)\n","    print(\"-\"*60)\n"]},{"cell_type":"code","source":["%pip -q install python-dotenv\n","\n","from pathlib import Path\n","from dotenv import load_dotenv, dotenv_values\n","import os\n","\n","PROJ = Path(\"/content/drive/MyDrive/rag_bio_project\")  # Êåâ‰Ω†ÁöÑÁúüÂÆûË∑ØÂæÑ\n","ENV_PATH = PROJ / \".env\"\n","\n","# ËØªÂèñÁúã‰∏Ä‰∏ãÊòØÂê¶ÁúüÁöÑÊãøÂà∞ key\n","cfg = dotenv_values(ENV_PATH)      # Âè™ËØªÂèñÔºå‰∏çÂÜôÁéØÂ¢É\n","print(\"[.env keys]\", list(cfg.keys()))\n","print(\"[OPENAI_API_KEY startswith sk?]\", str(cfg.get(\"OPENAI_API_KEY\",\"\"))[:7])\n","\n","# ÁúüÊ≠£ÂÜôÂÖ•ÂΩìÂâçËøõÁ®ãÁöÑÁéØÂ¢ÉÂèòÈáèÔºàoverride=True Ë¶ÜÁõñÂ∑≤ÊúâÂÄºÔºâ\n","load_dotenv(ENV_PATH, override=True)\n","\n","print(\"[verify] OPENAI_API_KEY set:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"],"metadata":{"id":"n8NRBSSL29Az","executionInfo":{"status":"aborted","timestamp":1759731894710,"user_tz":300,"elapsed":26566,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"n8NRBSSL29Az","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"30838bc7","metadata":{"id":"30838bc7","executionInfo":{"status":"aborted","timestamp":1759731894722,"user_tz":300,"elapsed":26577,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# 0) ÂÆâË£Ö‰æùËµñÔºàÂ¶ÇÂ∑≤ÂÆâË£ÖÂèØË∑≥ËøáÔºâ\n","%pip -q install -U numpy==1.26.4 chromadb==0.4.24 langchain==0.2.11 langchain-core==0.2.26         langchain-community==0.2.10 langchain-openai==0.1.17 pypdf tiktoken\n","# %pip -q install -U langchain-ollama  # Â¶ÇÊûúË¶ÅÁî®Êú¨Âú∞ Ollama Ê®°Âûã\n","print(\"Deps OK\")\n"]},{"cell_type":"code","execution_count":null,"id":"12abb7f8","metadata":{"id":"12abb7f8","executionInfo":{"status":"aborted","timestamp":1759731894723,"user_tz":300,"elapsed":26577,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# 1) Ë∑ØÂæÑËÆæÁΩÆÔºöÊåáÂêëÊàë‰ª¨ÂàöÂàöÁîüÊàêÁöÑÂ∑•Á®ãÁõÆÂΩï\n","from pathlib import Path\n","PROJ = Path(\"/content/drive/MyDrive/rag_bio_project\")\n","SRC = PROJ/\"src\"\n","INDEX_DIR = PROJ/\"index\"     # ÊåáÂêë‰Ω†ÁöÑ Chroma Á¥¢ÂºïÁõÆÂΩïÔºàDrive ‰∏≠ÂêåÂêç‰πüÂèØÊîπËøôÈáåÔºâ\n","import sys\n","sys.path.append(str(SRC))\n","print(\"Project:\", PROJ)\n","print(\"Index:\", INDEX_DIR)\n"]},{"cell_type":"code","execution_count":null,"id":"593ebeb0","metadata":{"id":"593ebeb0","executionInfo":{"status":"aborted","timestamp":1759731894748,"user_tz":300,"elapsed":26601,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# 2) ÂàóÂá∫ÂêëÈáèÂ∫ì collections ‰ª•Á°ÆËÆ§ÂèØËßÅ\n","from chromadb import PersistentClient\n","client = PersistentClient(path=str(INDEX_DIR))\n","cols = [c.name for c in client.list_collections()]\n","print(\"Collections:\", cols)\n"]},{"cell_type":"markdown","id":"d128a568","metadata":{"id":"d128a568"},"source":["## ‚úÖ Retriever ÂÜíÁÉüÊµãËØïÔºàÈ´òÈòàÂÄº + MMR + ÂàÜÁ∫ßÔºâ"]},{"cell_type":"code","source":["from pathlib import Path\n","from chromadb import PersistentClient\n","import json\n","\n","print(\"INDEX_DIR =\", INDEX_DIR)\n","client = PersistentClient(path=str(INDEX_DIR))\n","names = [c.name for c in client.list_collections()]\n","print(\"Collections:\", names)\n","\n","for n in names:\n","    coll = client.get_collection(n)\n","    meta = coll.metadata or {}\n","    emb = meta.get(\"embedding\")\n","    if isinstance(emb, str):\n","        try: emb = json.loads(emb)\n","        except: pass\n","    print(f\" - {n:<32} count={coll.count()}  embedding={emb}\")\n"],"metadata":{"id":"R_letEGl37Nk","executionInfo":{"status":"aborted","timestamp":1759731894764,"user_tz":300,"elapsed":26616,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"R_letEGl37Nk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from retriever import retrieve\n","\n","tmp = retrieve(\n","    persist_dir=str(INDEX_DIR),\n","    query_text=\"ÊùéÈùíÁöÑÂπ¥Êî∂ÂÖ•ÊòØÂ§öÂ∞ëÔºü\",\n","    k=10, strategy=\"similarity\",\n","    fetch_k=50,\n","    score_threshold=0.0,     # ÂÖ≥Èó≠ÈòàÂÄº\n",")\n","\n","scores = [round(x[\"score\"], 4) for x in tmp.get(\"items\", [])]\n","print(\"raw scores (top 10):\", scores[:10])\n"],"metadata":{"id":"g7cTmMlX4S_E","executionInfo":{"status":"aborted","timestamp":1759731894766,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"g7cTmMlX4S_E","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from chromadb import PersistentClient\n","from retriever import _embedder_from_coll_meta  # Êàë‰ª¨‰ª£Á†ÅÈáåÂ∑≤Êúâ\n","\n","client = PersistentClient(path=str(INDEX_DIR))\n","# Êåë‰∏Ä‰∏™‰Ω†Á°ÆËÆ§Êúâ‚ÄúÊùéÈùí‚ÄùÂÜÖÂÆπÁöÑÈõÜÂêàÂêç\n","cname = [n for n in names if \"LiQing\" in n or \"liqing\" in n.lower()][0]\n","coll = client.get_collection(cname)\n","\n","embedder = _embedder_from_coll_meta(coll.metadata or {})\n","q = \"ÊùéÈùí Âπ¥ Êî∂ÂÖ•\"\n","qvec = (embedder.embed_query(q)\n","        if hasattr(embedder, \"embed_query\")\n","        else embedder.embed_documents([q])[0])\n","\n","qr = coll.query(query_embeddings=[qvec], n_results=5,\n","                include=[\"documents\",\"distances\",\"metadatas\"])\n","print(\"docs:\", qr[\"documents\"][0][:2])\n","print(\"dists:\", qr[\"distances\"][0][:2])\n"],"metadata":{"id":"UpU3dDMN4cc4","executionInfo":{"status":"aborted","timestamp":1759731894767,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"UpU3dDMN4cc4","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"599dd73b","metadata":{"id":"599dd73b","executionInfo":{"status":"aborted","timestamp":1759731894768,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","from retriever import retrieve\n","question = \"ÊùéÈùíÁöÑÂπ¥Êî∂ÂÖ•ÊòØÂ§öÂ∞ëÔºü\"\n","res = retrieve(persist_dir=str(INDEX_DIR), query_text=question,\n","               k=5, strategy=\"mmr\", strictness=\"strict\")\n","print(\"Route:\", res.get(\"route\"))\n","for it in res.get(\"items\", [])[:3]:\n","    print(it[\"grade\"], f\"{it['score']:.3f}\", \"src:\", (it[\"metadata\"] or {}).get(\"source\"))\n"]},{"cell_type":"markdown","id":"9b83ff81","metadata":{"id":"9b83ff81"},"source":["## üß© MiddlewareÔºöËßíËâ≤ËØÜÂà´ / ÊÄÄÁñëÂ∫¶ÔºàÂç†‰ΩçÔºâ / ‰∫∫Ê†ºÔºàÂç†‰ΩçÔºâ / Â§çÊü•ÔºàÂç†‰ΩçÔºâ"]},{"cell_type":"code","execution_count":null,"id":"5738f107","metadata":{"id":"5738f107","executionInfo":{"status":"aborted","timestamp":1759731894770,"user_tz":300,"elapsed":26618,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","# from middleware import detect_characters_from_question, VerificationConfig, verify_answer_against_context\n","# det = detect_characters_from_question(\"ËØ∑ÊØîËæÉLiQing‰∏éWangMuÁöÑÊî∂ÂÖ•\", persist_dir=str(INDEX_DIR))\n","# print(\"Role detection:\", det)\n","# # Â§çÊü•Âç†‰ΩçÊºîÁ§∫ÔºàÈªòËÆ§‰∏çÂêØÁî®Ôºâ\n","# vres = verify_answer_against_context(\"dummy answer\", res.get(\"items\", []), VerificationConfig(enabled=False))\n","# print(\"Verification (disabled):\", vres)\n"]},{"cell_type":"markdown","id":"934592ca","metadata":{"id":"934592ca"},"source":["## üß† Prompt Ëá™Âä®ÈÄâÊã© + È¢ÑËßà"]},{"cell_type":"code","execution_count":null,"id":"025628fa","metadata":{"id":"025628fa","executionInfo":{"status":"aborted","timestamp":1759731894771,"user_tz":300,"elapsed":26618,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["\n","from prompting import build_prompt_messages_auto\n","msgs, info = build_prompt_messages_auto(\"What is LiQing's annual income?\", res)\n","print(\"Mode:\", info[\"mode\"])\n","print(\"System msg preview:\", msgs[0].content[:160])\n"]},{"cell_type":"markdown","id":"e029bff5","metadata":{"id":"e029bff5"},"source":["## üöÄ ‰∏ÄÈîÆÊµÅÊ∞¥Á∫ø DemoÔºàÈúÄË¶Å API KeyÔºâ"]},{"cell_type":"code","execution_count":null,"id":"34c4db4b","metadata":{"id":"34c4db4b","executionInfo":{"status":"aborted","timestamp":1759731894771,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[],"source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # Âª∫ËÆÆÂÖàÁî® mediumÔºåÁ°ÆËÆ§Âè¨ÂõûÂêéÂÜçË∞ÉÈ´ò\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # ÂèØÈÄâÔºöÂú®ËøôÈáåÂºÄÂêØ/Ë∞ÉÊï¥‰∏≠Èó¥Â±Ç\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"ÊùéÈùíÁöÑÂπ¥Êî∂ÂÖ•ÊòØÂ§öÂ∞ëÔºü\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"]},{"cell_type":"code","source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # Âª∫ËÆÆÂÖàÁî® mediumÔºåÁ°ÆËÆ§Âè¨ÂõûÂêéÂÜçË∞ÉÈ´ò\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # ÂèØÈÄâÔºöÂú®ËøôÈáåÂºÄÂêØ/Ë∞ÉÊï¥‰∏≠Èó¥Â±Ç\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"ÊùéÈùí, ‰Ω†ÁªìÂ©ö‰∫ÜÂêó?‰ª•ÊùéÈùíÁöÑÂè£ÂêªÂõûÁ≠î\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"],"metadata":{"id":"Gj7QbqGoJ5QN","executionInfo":{"status":"aborted","timestamp":1759731894772,"user_tz":300,"elapsed":26617,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"Gj7QbqGoJ5QN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","print(\"OPENAI_API_KEY exists:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n","print(\"OPENAI_ORG:\", os.getenv(\"OPENAI_ORG\"))\n"],"metadata":{"id":"upaIbVgo-5lf","executionInfo":{"status":"aborted","timestamp":1759731894775,"user_tz":300,"elapsed":26619,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"id":"upaIbVgo-5lf","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}