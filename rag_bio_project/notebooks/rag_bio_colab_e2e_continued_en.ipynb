{"cells":[{"cell_type":"markdown","id":"1f15465b","metadata":{"id":"1f15465b"},"source":["# English I/O Edition\n","This notebook is an **English-only I/O clone** of your original. All prompts to the model are forced to English, and the model is instructed to **reply in English only**.\n","\n","**Note:** The code, structure, and execution order remain identical to your original notebook."]},{"cell_type":"code","execution_count":null,"id":"b381baa3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b381baa3","executionInfo":{"status":"ok","timestamp":1759731957581,"user_tz":300,"elapsed":16,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}},"outputId":"fb012b34-f9d5-43c0-9640-b9dba8578bac"},"outputs":[{"output_type":"stream","name":"stdout","text":["[EN-SHIM] Skipped patching prompting module: No module named 'prompting'\n"]}],"source":["# --- English I/O shim (non-invasive) ---\n","import types\n","\n","def _force_english_messages(messages):\n","    \"\"\"Ensure the first system message enforces English-only behavior.\"\"\"\n","    sys_en = (\n","        \"You are a helpful assistant. \"\n","        \"Use concise, clear **English** only. \"\n","        \"If the provided context is insufficient, say you don't know. \"\n","        \"Do not fabricate facts.\"\n","    )\n","    try:\n","        # LangChain-style list of dicts\n","        if isinstance(messages, list) and messages:\n","            if isinstance(messages[0], dict) and messages[0].get(\"role\") == \"system\":\n","                messages[0][\"content\"] = sys_en + \"\\n\\n\" + str(messages[0].get(\"content\", \"\"))\n","            else:\n","                messages = [{\"role\": \"system\", \"content\": sys_en}] + messages\n","    except Exception:\n","        pass\n","    return messages\n","\n","# Monkey-patch prompting.build_prompt_messages_auto if available\n","try:\n","    import prompting as _prompting_mod\n","    _orig_bpm = _prompting_mod.build_prompt_messages_auto\n","\n","    def build_prompt_messages_auto(*args, **kwargs):\n","        msgs, info = _orig_bpm(*args, **kwargs)\n","        msgs = _force_english_messages(msgs)\n","        if isinstance(info, dict):\n","            info[\"lang\"] = \"en\"\n","        return msgs, info\n","\n","    _prompting_mod.build_prompt_messages_auto = build_prompt_messages_auto\n","    print(\"[EN-SHIM] build_prompt_messages_auto patched to enforce English I/O.\")\n","except Exception as e:\n","    print(\"[EN-SHIM] Skipped patching prompting module:\", e)\n","\n","# Expose helper for any direct LLM calls you might add later\n","force_english_messages = _force_english_messages\n"]},{"cell_type":"markdown","id":"tEgHi1sbaSrQ","metadata":{"id":"tEgHi1sbaSrQ"},"source":["# ğŸ“˜ RAG Biography (Colab Edition)\n","\n","**One-click end-to-end:** install deps â†’ mount Drive â†’ set project path â†’ generate sample PDFs â†’ load â†’ split â†’ embed â†’ build Chroma collections `(username)_(character_name)` â†’ quick retrieval.\n","\n","> Default embeddings: **HuggingFace BGE small zh v1.5** (no API needed). You can switch to OpenAI/DashScope by editing the *Embeddings* cell.\n"]},{"cell_type":"code","execution_count":null,"id":"rXkdWBCNaSrR","metadata":{"id":"rXkdWBCNaSrR"},"outputs":[],"source":["# # # ===== Clean + Pin =====\n","# import sys, subprocess, IPython\n","\n","# def sh(cmd):\n","#     print(\">>\", cmd)\n","#     subprocess.run(cmd, shell=True, check=False)\n","\n","# # 1) æ¸… spacy å®¶æ—ï¼ˆä¼šæ‹‰ weasel/srsly/wasabiï¼Œå¸¸æ…ç‰ˆæœ¬ï¼‰\n","# sh(\"pip -q uninstall -y spacy thinc catalogue srsly cymem preshed murmurhash wasabi blis typer langcodes || true\")\n","\n","# # 2) å›ºå®š requestsï¼ˆé¿å…å’Œç³»ç»ŸåŒ…å†²çªï¼‰\n","# sh(\"pip -q install -U requests==2.32.2\")\n","\n","# # 3) å…³é”®ï¼šå¼ºåˆ¶æŠŠ numpy é™åˆ° 1.26.4ï¼ˆå¹¶é¿å…å†æ¬¡è¢«åˆ«çš„åŒ…å‡å›å»ï¼‰\n","# sh('pip -q install --force-reinstall --no-build-isolation \"numpy==1.26.4\"')\n","\n","# # 4) ä½ çš„ä¾èµ–\n","# sh(\"pip -q install -U chromadb==0.4.24 \"\n","#    \"langchain==0.2.11 langchain-core==0.2.26 langchain-community==0.2.10 \"\n","#    \"langchain-openai==0.1.17 pypdf tiktoken\")\n","\n","# # 5) æ ¡éªŒä¾èµ–å†²çªï¼ˆå¯é€‰ï¼‰\n","# sh(\"pip -q check || true\")\n","\n","# sh(\"pip -q install -U reportlab python-dotenv\")\n","\n","# print(\"\\nâœ… å®‰è£…å®Œæˆï¼šå³å°†é‡å¯å†…æ ¸ä»¥åˆ‡æ¢åˆ° NumPy 1.26.4 ...\")\n","# IPython.get_ipython().kernel.do_shutdown(restart=True)  # è‡ªåŠ¨é‡å¯\n"]},{"cell_type":"code","execution_count":null,"id":"v6qY0wVfaSrR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6qY0wVfaSrR","outputId":"4a827d2e-b19c-4bf9-cce8-dfb35668b28a","executionInfo":{"status":"ok","timestamp":1759731958173,"user_tz":300,"elapsed":581,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Project path: /content/drive/MyDrive/rag_bio_project\n","âœ… Folders ready.\n"]}],"source":["from google.colab import drive\n","from pathlib import Path\n","drive.mount('/content/drive')\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","print('Project path:', PROJ)\n","\n","for d in [PROJ, PROJ/'src', PROJ/'data_pdfs', PROJ/'data_txt', PROJ/'index', PROJ/'notebooks']:\n","    d.mkdir(parents=True, exist_ok=True)\n","print('âœ… Folders ready.')"]},{"cell_type":"code","execution_count":null,"id":"VUOy578IaSrR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VUOy578IaSrR","outputId":"5c9d1b80-93dc-4088-f5ed-e7e197478fc8","executionInfo":{"status":"ok","timestamp":1759731958188,"user_tz":300,"elapsed":15,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[SKIP ] /content/drive/MyDrive/rag_bio_project/src/loader.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/splitter.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/embeddings.py\n","[SKIP ] /content/drive/MyDrive/rag_bio_project/src/vectorstore.py\n","âœ… src/ files ensured.\n"]}],"source":["from pathlib import Path\n","import textwrap\n","\n","SRC = PROJ/'src'\n","\n","def ensure_file(path: Path, content: str):\n","    if not path.exists():\n","        path.write_text(textwrap.dedent(content), encoding='utf-8')\n","        print('[WRITE]', path)\n","    else:\n","        print('[SKIP ]', path)\n","\n","# --- loader.py ---\n","ensure_file(SRC/'loader.py', '''\n","from pathlib import Path\n","from typing import List, Optional, Iterable, Dict\n","import hashlib, re, time\n","from urllib.parse import urlsplit, urlunsplit\n","from langchain_core.documents import Document\n","from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\n","def _normalize_text(s: str) -> str:\n","    if not s: return ''\n","    s = s.replace('\\ufeff', '').replace('\\xa0', ' ')\n","    s = re.sub(r'[ \\t]+', ' ', s)\n","    s = re.sub(r'\\n{3,}', '\\n\\n', s)\n","    return s.strip()\n","def _clean_url(u: str) -> str:\n","    u = u.strip().replace(' ', '')\n","    parts = list(urlsplit(u))\n","    if not parts[0]: parts[0] = 'https'\n","    return urlunsplit(parts)\n","def _doc_hash(content: str) -> str:\n","    return hashlib.md5(content.encode('utf-8', errors='ignore')).hexdigest()[:16]\n","def _filter_content(text: str, min_chars: int, max_chars: Optional[int]) -> bool:\n","    n = len(text)\n","    if n < min_chars: return False\n","    if max_chars is not None and n > max_chars: return False\n","    return True\n","def load_sources(pdf_dir: str='data_pdfs', txt_dir: Optional[str]='data_txt', urls: Optional[List[str]] = None,\n","                 recursive: bool=True, txt_extensions: Iterable[str]=( '.txt', '.md'), txt_encoding: str='utf-8',\n","                 pages: Optional[str]=None, min_chars: int=50, max_chars: Optional[int]=None,\n","                 headers: Optional[Dict[str,str]] = None, timeout: int=15, max_retries: int=2) -> List[Document]:\n","    docs: List[Document] = []\n","    total_raw = 0\n","    pdir = Path(pdf_dir)\n","    if pdir.exists():\n","        for p in (pdir.rglob('*.pdf') if recursive else pdir.glob('*.pdf')):\n","            try:\n","                loader = PyPDFLoader(str(p))\n","                loaded = loader.load()\n","                if pages:\n","                    parts = [None if x in ('', 'None', None) else int(x) for x in pages.split(':')]\n","                    start = parts[0] if len(parts)>0 else None\n","                    stop  = parts[1] if len(parts)>1 else None\n","                    step  = parts[2] if len(parts)>2 else None\n","                    loaded = [d for d in loaded[slice(start, stop, step)]]\n","                for d in loaded:\n","                    d.page_content = _normalize_text(d.page_content)\n","                    d.metadata['source'] = str(p)\n","                    d.metadata['source_type'] = 'pdf'\n","                    d.metadata['loader_info'] = {'type':'PyPDFLoader','pages':pages}\n","                    if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                total_raw += len(loaded)\n","            except Exception as e:\n","                print(f'[WARN] Skip PDF {p}: {e}')\n","    if txt_dir:\n","        tdir = Path(txt_dir)\n","        if tdir.exists():\n","            exts = {e.lower() for e in txt_extensions}\n","            for p in (tdir.rglob('*') if recursive else tdir.glob('*')):\n","                if p.is_file() and p.suffix.lower() in exts:\n","                    try:\n","                        loader = TextLoader(str(p), encoding=txt_encoding)\n","                        loaded = loader.load()\n","                        for d in loaded:\n","                            d.page_content = _normalize_text(d.page_content)\n","                            d.metadata['source'] = str(p)\n","                            d.metadata['source_type'] = 'txt'\n","                            d.metadata['loader_info'] = {'type':'TextLoader','encoding':txt_encoding}\n","                            if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                        total_raw += len(loaded)\n","                    except Exception as e:\n","                        print(f'[WARN] Skip TXT {p}: {e}')\n","    if urls:\n","        urls = [u for u in (urls or []) if isinstance(u, str) and u.strip()]\n","        for u in urls:\n","            url = _clean_url(u)\n","            tries = 0\n","            while True:\n","                try:\n","                    loader = WebBaseLoader(url, requests_kwargs={'headers': headers or {'User-Agent':'Mozilla/5.0 (RAG-Loader/1.0)'}, 'timeout': timeout})\n","                    loaded = loader.load()\n","                    for d in loaded:\n","                        d.page_content = _normalize_text(d.page_content)\n","                        d.metadata['source'] = url\n","                        d.metadata['source_type'] = 'web'\n","                        d.metadata['loader_info'] = {'type':'WebBaseLoader','timeout':timeout,'headers':bool(headers)}\n","                        if _filter_content(d.page_content, min_chars, max_chars): docs.append(d)\n","                    total_raw += len(loaded)\n","                    break\n","                except Exception as e:\n","                    tries += 1\n","                    if tries > max_retries:\n","                        print(f'[WARN] Skip URL {url} after {max_retries} retries: {e}'); break\n","                    print(f'[INFO] Retry {tries}/{max_retries} for URL {url} due to: {e}')\n","    seen = set(); uniq: List[Document] = []\n","    for d in docs:\n","        h = _doc_hash(d.page_content)\n","        key = (d.metadata.get('source'), h)\n","        if key not in seen:\n","            seen.add(key); uniq.append(d)\n","    print(f'[INFO] Loader complete. raw={total_raw}, kept={len(uniq)}')\n","    return uniq\n","''')\n","\n","# --- splitter.py ---\n","ensure_file(SRC/'splitter.py', '''\n","from typing import List, Dict\n","from dataclasses import dataclass\n","from langchain_core.documents import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","@dataclass\n","class SplitterProfile:\n","    chunk_size: int; chunk_overlap: int; separators: List[str]\n","PDF_PROFILE = SplitterProfile(1200, 200, ['\\\\n\\\\n','\\\\n','ã€‚','ï¼','ï¼Ÿ','.', '!', '?', ' ', ''])\n","TXT_PROFILE = SplitterProfile(1000, 150, ['\\\\n\\\\n','\\\\n','.', '?', '!', 'ã€‚','ï¼Ÿ','ï¼',' ', ''])\n","WEB_PROFILE = SplitterProfile(900, 150,  ['\\\\n\\\\n','\\\\n','ã€‚','ï¼','ï¼Ÿ','.', '!', '?', ' ', ''])\n","PROFILE_MAP: Dict[str, SplitterProfile] = {'pdf':PDF_PROFILE,'txt':TXT_PROFILE,'web':WEB_PROFILE}\n","def _build_splitter(p: SplitterProfile) -> RecursiveCharacterTextSplitter:\n","    overlap = p.chunk_overlap if p.chunk_overlap < p.chunk_size else max(0, min(p.chunk_size//5, 200))\n","    if overlap != p.chunk_overlap: print(f'[WARN] overlap >= size; fallback to {overlap}')\n","    return RecursiveCharacterTextSplitter(chunk_size=p.chunk_size, chunk_overlap=overlap, separators=p.separators)\n","def split_documents_type_aware(docs: List[Document], default_type: str='pdf', verbose: bool=True) -> List[Document]:\n","    if verbose: print(f'[INFO] Type-aware splitting started. total_docs={len(docs)}, default_type={default_type}')\n","    splitter_cache: Dict[str, RecursiveCharacterTextSplitter] = {}\n","    def get_splitter(kind: str):\n","        typ = (kind or '').lower(); typ = typ if typ in PROFILE_MAP else default_type\n","        if typ not in splitter_cache:\n","            splitter_cache[typ] = _build_splitter(PROFILE_MAP[typ])\n","            if verbose:\n","                p = PROFILE_MAP[typ]; print(f\"[INFO] Splitter ready for type='{typ}' (size={p.chunk_size}, overlap={p.chunk_overlap})\")\n","        return splitter_cache[typ]\n","    out: List[Document] = []; per_source_index: Dict[str,int] = {}\n","    for d in docs:\n","        stype = (d.metadata.get('source_type') or default_type).lower()\n","        splitter = get_splitter(stype)\n","        chunks = splitter.split_documents([d])\n","        source_key = str(d.metadata.get('source', 'unknown'))\n","        start_idx = per_source_index.get(source_key, 0)\n","        for i, c in enumerate(chunks):\n","            c.metadata = dict(d.metadata) | {'chunk_index': start_idx + i, 'splitter_profile': stype}\n","            out.append(c)\n","        per_source_index[source_key] = start_idx + len(chunks)\n","        if verbose: print(f\"[INFO] Split {stype} source={source_key} -> {len(chunks)} chunks (acc={len(out)})\")\n","    if verbose: print(f'[INFO] Type-aware splitting done. total_chunks={len(out)}')\n","    return out\n","''')\n","\n","# --- embeddings.py ---\n","ensure_file(SRC/'embeddings.py', '''\n","import os, re, time, hashlib\n","from dataclasses import dataclass\n","from typing import List, Tuple, Dict, Optional\n","from langchain_core.documents import Document\n","from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.embeddings import DashScopeEmbeddings\n","from chromadb import PersistentClient\n","@dataclass\n","class EmbeddingConfig:\n","    provider: str = 'hf'\n","    model: str = 'BAAI/bge-small-zh-v1.5'\n","    normalize: bool = True\n","def _sanitize_name(s: str, fallback_prefix: str='user') -> str:\n","    s2 = re.sub(r'[^a-zA-Z0-9_]+', '_', str(s or '')).strip('_')\n","    return s2 or f\"{fallback_prefix}_{int(time.time())}\"\n","def _doc_id(meta: Dict, idx: int) -> str:\n","    base = f\"{meta.get('source','unknown')}|{meta.get('chunk_index', idx)}\"\n","    return hashlib.md5(base.encode('utf-8', errors='ignore')).hexdigest()[:12] + f'_{idx}'\n","def build_embeddings(cfg: EmbeddingConfig):\n","    prov = cfg.provider.lower()\n","    if prov == 'hf':\n","        print(f\"[INFO] Using HF embeddings: {cfg.model} (normalize={cfg.normalize})\")\n","        return HuggingFaceBgeEmbeddings(model_name=cfg.model, encode_kwargs={'normalize_embeddings': cfg.normalize})\n","    elif prov == 'openai':\n","        if not os.getenv('OPENAI_API_KEY'): raise RuntimeError('OPENAI_API_KEY is not set.')\n","        model_name = cfg.model or 'text-embedding-3-small'\n","        print(f\"[INFO] Using OpenAI embeddings: {model_name}\")\n","        return OpenAIEmbeddings(model=model_name)\n","    elif prov == 'dashscope':\n","        if not os.getenv('DASHSCOPE_API_KEY'): raise RuntimeError('DASHSCOPE_API_KEY is not set.')\n","        model_name = cfg.model or 'text-embedding-v1'\n","        print(f\"[INFO] Using DashScope embeddings: {model_name}\")\n","        return DashScopeEmbeddings(model=model_name)\n","    else:\n","        raise ValueError(f'Unknown provider: {cfg.provider}')\n","def compute_vectors_once(chunks: List[Document], embedder):\n","    texts = [c.page_content for c in chunks]\n","    metas = [dict(c.metadata or {}) for c in chunks]\n","    ids = [_doc_id(m, i) for i, m in enumerate(metas)]\n","    print(f\"[INFO] Computing embeddings for {len(texts)} chunks...\")\n","    vectors = embedder.embed_documents(texts)\n","    if not vectors or not vectors[0]: raise RuntimeError('Empty embeddings returned.')\n","    dim = len(vectors[0])\n","    emb_info = {'provider': type(embedder).__name__, 'dim': dim}\n","    print(f\"[INFO] Embedding dim={dim}\")\n","    return ids, texts, metas, vectors, emb_info\n","def persist_vectorstores_for_characters(ids, texts, metas, vectors, persist_dir, username, character_names, emb_meta, collection_prefix: Optional[str]=None):\n","    user_tag = _sanitize_name(username, 'user')\n","    created = []\n","    client: PersistentClient = PersistentClient(path=persist_dir)\n","    for cname in character_names:\n","        char_tag = _sanitize_name(cname, 'char')\n","        base_name = f\"{user_tag}_{char_tag}\"\n","        coll_name = base_name if not collection_prefix else _sanitize_name(f\"{collection_prefix}_{base_name}\", 'coll')\n","        print(f\"[INFO] Creating/updating collection: {coll_name}\")\n","        coll = client.get_or_create_collection(name=coll_name, metadata={'embedding': emb_meta})\n","        B = 256\n","        for i in range(0, len(ids), B):\n","            coll.add(ids=ids[i:i+B], documents=texts[i:i+B], metadatas=metas[i:i+B], embeddings=vectors[i:i+B])\n","        print(f\"[INFO] Collection '{coll_name}' upserted with {len(ids)} items.\")\n","        created.append(coll_name)\n","    print(f\"[INFO] Done. Created/updated {len(created)} collections.\")\n","    return created\n","def build_embeddings_and_vectorstores(chunks: List[Document], username: str, character_names: List[str], persist_dir: str='index', emb_cfg: EmbeddingConfig = EmbeddingConfig()):\n","    if not character_names: raise ValueError('character_names must not be empty.')\n","    embedder = build_embeddings(emb_cfg)\n","    ids, texts, metas, vectors, emb_info = compute_vectors_once(chunks, embedder)\n","    return persist_vectorstores_for_characters(ids, texts, metas, vectors, persist_dir, username, character_names, {'provider':emb_cfg.provider,'model':emb_cfg.model,'dim':emb_info['dim'],'normalize':emb_cfg.normalize})\n","''')\n","\n","# --- vectorstore.py ---\n","ensure_file(SRC/'vectorstore.py', '''\n","from chromadb import PersistentClient\n","def get_collection(persist_dir: str, collection_name: str):\n","    client = PersistentClient(path=persist_dir)\n","    return client.get_collection(collection_name)\n","def quick_query(persist_dir: str, collection_name: str, query_text: str, n_results: int=5):\n","    coll = get_collection(persist_dir, collection_name)\n","    return coll.query(query_texts=[query_text], n_results=n_results, include=['documents','metadatas','distances'])\n","''')\n","\n","print('âœ… src/ files ensured.')"]},{"cell_type":"code","execution_count":null,"id":"bqtMUD1CaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqtMUD1CaSrS","outputId":"92ff71d6-b69a-4bf9-f912-7075e1f5c2e9","executionInfo":{"status":"ok","timestamp":1759731963571,"user_tz":300,"elapsed":5382,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Added to sys.path: /content/drive/MyDrive/rag_bio_project/src\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Imports OK\n"]}],"source":["import sys\n","from pathlib import Path\n","CANDIDATES = [PROJ, Path.cwd(), Path('/content/drive/MyDrive/rag_bio_project')]\n","project_src=None\n","for base in CANDIDATES:\n","    src = base/'src'\n","    if src.exists() and (src/'loader.py').exists():\n","        project_src=src.resolve(); break\n","assert project_src, 'src/ not found. Check PROJ path.'\n","sys.path.insert(0, str(project_src))\n","print('[INFO] Added to sys.path:', project_src)\n","\n","from loader import load_sources\n","from splitter import split_documents_type_aware\n","from embeddings import EmbeddingConfig, build_embeddings_and_vectorstores\n","from vectorstore import quick_query\n","print('âœ… Imports OK')"]},{"cell_type":"code","execution_count":null,"id":"B9t-jeBCaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9t-jeBCaSrS","outputId":"c3c93a9f-0c1d-4fd8-b0ea-6c5bae448935","executionInfo":{"status":"ok","timestamp":1759731963627,"user_tz":300,"elapsed":43,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] ENV loaded. OPENAI_API_KEY exists: False\n","[INFO] PDFs generated at /content/drive/MyDrive/rag_bio_project/data_pdfs\n","[INFO] TXT samples ensured at data_txt\n"]}],"source":["from reportlab.lib.pagesizes import letter\n","from reportlab.pdfgen import canvas\n","from dotenv import load_dotenv\n","import os\n","\n","DATA_PDFS = PROJ/'data_pdfs'\n","DATA_TXT  = PROJ/'data_txt'\n","INDEX_DIR = PROJ/'index'\n","ENV_PATH  = PROJ/' .env'\n","load_dotenv(ENV_PATH)\n","print('[INFO] ENV loaded. OPENAI_API_KEY exists:', bool(os.getenv('OPENAI_API_KEY')))\n","\n","def make_pdf(path, title, lines):\n","    c = canvas.Canvas(str(path), pagesize=letter)\n","    width, height = letter\n","    y = height - 72\n","    c.setFont('Times-Roman', 12)\n","    c.drawString(72, y, title); y -= 24\n","    for ln in lines:\n","        for seg in ln.split('\\n'):\n","            c.drawString(72, y, seg); y -= 18\n","            if y < 72:\n","                c.showPage(); y = height - 72; c.setFont('Times-Roman', 12)\n","    c.save()\n","\n","make_pdf(DATA_PDFS/'liqing.pdf', 'Li Qing Biography', [\n","    'Born in Suzhou in 1990; studied Economics and became a product manager.',\n","    'Finance: ~350k CNY income; index funds; medium risk preference.',\n","    'Marriage: married in 2018; one daughter in 2022.',\n","    'Experience: 2020-2022 SEA market localization projects.'\n","])\n","make_pdf(DATA_PDFS/'wangmu.pdf', 'Wang Mu Biography', [\n","    'Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations.',\n","    'Finance: ~200k CNY income; house + money-market funds; conservative.',\n","    'Marriage: single; passionate about education equity.',\n","    'Experience: 2015-2019 broker risk control; compliance expertise.'\n","])\n","print('[INFO] PDFs generated at', DATA_PDFS)\n","\n","from pathlib import Path\n","\n","DATA_TXT = Path(\"data_txt\")\n","DATA_TXT.mkdir(parents=True, exist_ok=True)\n","\n","liqing = \"\\n\".join([\n","    \"Born in Suzhou in 1990; studied Economics and became a product manager.\",\n","    \"Finance: ~350k CNY income; index funds; medium risk preference.\",\n","    \"Marriage: married in 2018; one daughter in 2022.\",\n","    \"Experience: 2020-2022 SEA market localization projects.\",\n","])\n","(DATA_TXT / \"liqing.txt\").write_text(liqing, encoding=\"utf-8\")\n","\n","wangmu = \"\\n\".join([\n","    \"Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations.\",\n","    \"Finance: ~200k CNY income; house + money-market funds; conservative.\",\n","    \"Marriage: single; passionate about education equity.\",\n","    \"Experience: 2015-2019 broker risk control; compliance expertise.\",\n","])\n","(DATA_TXT / \"wangmu.txt\").write_text(wangmu, encoding=\"utf-8\")\n","\n","print('[INFO] TXT samples ensured at', DATA_TXT)"]},{"cell_type":"code","execution_count":null,"id":"edEvIZTUeyMj","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edEvIZTUeyMj","outputId":"73c2c368-c717-446d-fc0d-5e11cdc84e5f","executionInfo":{"status":"ok","timestamp":1759731963678,"user_tz":300,"elapsed":38,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ” loader.py patched\n","âœ” embeddings.py patched\n","âœ… Patch applied & modules reloaded.\n"]}],"source":["from pathlib import Path\n","import importlib, re, json, sys\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","SRC = PROJ/'src'\n","assert SRC.exists(), f\"src not found: {SRC}\"\n","\n","# 1) patch loader.py: make loader_info a JSON string (not a dict)\n","lp = SRC/'loader.py'\n","ls = lp.read_text(encoding='utf-8')\n","\n","if 'import json' not in ls:\n","    ls = ls.replace('from urllib.parse import urlsplit, urlunsplit',\n","                    'from urllib.parse import urlsplit, urlunsplit\\nimport json')\n","\n","# PyPDFLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'PyPDFLoader','pages':pages}, ensure_ascii=False)\",\n","    ls\n",")\n","# TextLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'TextLoader','encoding':txt_encoding}, ensure_ascii=False)\",\n","    ls,\n","    count=1  # only replace the TXT occurrence once\n",")\n","# WebBaseLoader\n","ls = re.sub(\n","    r\"d\\.metadata\\['loader_info'\\]\\s*=\\s*\\{[^}]+\\}\",\n","    \"d.metadata['loader_info'] = json.dumps({'type':'WebBaseLoader','timeout':timeout,'headers':bool(headers)}, ensure_ascii=False)\",\n","    ls,\n","    count=1  # only replace the Web occurrence once\n",")\n","\n","lp.write_text(ls, encoding='utf-8')\n","print(\"âœ” loader.py patched\")\n","\n","# 2) patch embeddings.py: (a) collection metadata -> JSON string (b) sanitize per-doc metadatas before add()\n","ep = SRC/'embeddings.py'\n","es = ep.read_text(encoding='utf-8')\n","\n","if 'import json' not in es:\n","    es = es.replace('from chromadb import PersistentClient',\n","                    'from chromadb import PersistentClient\\nimport json')\n","\n","# add a sanitizer\n","if '_sanitize_meta_for_chroma' not in es:\n","    es = es.replace(\n","        'def compute_vectors_once(chunks: List[Document], embedder):',\n","        \"\"\"def _sanitize_meta_for_chroma(m: dict) -> dict:\n","    out = {}\n","    for k, v in (m or {}).items():\n","        if isinstance(v, (str, int, float, bool)):\n","            out[k] = v\n","        else:\n","            out[k] = json.dumps(v, ensure_ascii=False)\n","    return out\n","\n","def compute_vectors_once(chunks: List[Document], embedder):\"\"\"\n","    )\n","\n","# ensure collection metadata uses JSON string\n","es = es.replace(\n","    \"metadata={'embedding': emb_meta}\",\n","    \"metadata={'embedding': json.dumps(emb_meta, ensure_ascii=False)}\"\n",")\n","\n","# sanitize metas list before coll.add\n","es = re.sub(\n","    r\"for i in range\\(0, len\\(ids\\), B\\):\\s*\"\n","    r\"coll\\.add\\(ids=ids\\[i:i\\+B\\], documents=texts\\[i:i\\+B\\], metadatas=metas\\[i:i\\+B\\], embeddings=vectors\\[i:i\\+B\\]\\)\",\n","    \"for i in range(0, len(ids), B):\\n\"\n","    \"            batch_metas = [ _sanitize_meta_for_chroma(m) for m in metas[i:i+B] ]\\n\"\n","    \"            coll.add(ids=ids[i:i+B], documents=texts[i:i+B], metadatas=batch_metas, embeddings=vectors[i:i+B])\",\n","    es\n",")\n","\n","ep.write_text(es, encoding='utf-8')\n","print(\"âœ” embeddings.py patched\")\n","\n","# 3) reload modules\n","sys.path.insert(0, str(SRC))\n","import loader, embeddings\n","import importlib\n","importlib.reload(loader)\n","importlib.reload(embeddings)\n","from embeddings import EmbeddingConfig, build_embeddings_and_vectorstores\n","print(\"âœ… Patch applied & modules reloaded.\")\n"]},{"cell_type":"code","source":["# # åˆ é™¤å•ä¸ª/å¤šä¸ªæ—§ collectionï¼ˆæ³¨æ„ï¼šä¸å¯æ¢å¤ï¼‰\n","# from chromadb import PersistentClient\n","# client = PersistentClient(path=str(INDEX_DIR))\n","# for coll in client.list_collections():\n","#     print(\" -\", coll.name)\n","# # ä¾‹å¦‚åˆ é™¤æŒ‡å®šæ—§é›†åˆ\n","# client.delete_collection(\"demo_user_LiQing\")\n","# client.delete_collection(\"demo_user_WangMu\")\n"],"metadata":{"id":"hZT1_YCiINW8"},"id":"hZT1_YCiINW8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"aEENRsoyaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aEENRsoyaSrS","outputId":"c8a91387-ea88-41e4-ac84-3c478d09eeed","executionInfo":{"status":"ok","timestamp":1759731981161,"user_tz":300,"elapsed":17466,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Loader complete. raw=4, kept=4, pdf_dir=/content/drive/MyDrive/rag_bio_project/data_pdfs, txt_dir=data_txt, urls=0\n","[INFO] loaded docs: 4\n","[INFO] Type-aware splitting started. total_docs=4, default_type=pdf\n","[INFO] Splitter ready for type='pdf' (size=1200, overlap=200)\n","[INFO] Split pdf source=/content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf -> 1 chunks (acc=1)\n","[INFO] Split pdf source=/content/drive/MyDrive/rag_bio_project/data_pdfs/wangmu.pdf -> 1 chunks (acc=2)\n","[INFO] Splitter ready for type='txt' (size=1000, overlap=150)\n","[INFO] Split txt source=data_txt/wangmu.txt -> 1 chunks (acc=3)\n","[INFO] Split txt source=data_txt/liqing.txt -> 1 chunks (acc=4)\n","[INFO] Type-aware splitting done. total_chunks=4\n","[INFO] chunks: 4\n","[INFO] Using HF embeddings: BAAI/bge-m3 (normalize=True)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Computing embeddings for 4 chunks...\n","[INFO] Embedding dim=1024\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Creating/updating collection: demo_user_LiQing\n","[INFO] Collection 'demo_user_LiQing' upserted with 4 items.\n","[INFO] Creating/updating collection: demo_user_WangMu\n","[INFO] Collection 'demo_user_WangMu' upserted with 4 items.\n","[INFO] Done. Created/updated 2 collections.\n","[INFO] Collections created: ['demo_user_LiQing', 'demo_user_WangMu']\n"]}],"source":["docs = load_sources(pdf_dir=str(DATA_PDFS), txt_dir=str(DATA_TXT), urls=None)\n","print('[INFO] loaded docs:', len(docs))\n","chunks = split_documents_type_aware(docs, default_type='pdf', verbose=True)\n","print('[INFO] chunks:', len(chunks))\n","\n","username = 'demo_user'\n","character_names = ['LiQing', 'WangMu']\n","\n","emb_cfg = EmbeddingConfig(\n","    provider=\"hf\",\n","    model=\"BAAI/bge-m3\",         # â† å¤šè¯­ç§æ¨¡å‹\n","    normalize=True\n",")\n","\n","\n","# Optional:\n","# emb_cfg = EmbeddingConfig(provider='openai', model='text-embedding-3-small')\n","# emb_cfg = EmbeddingConfig(provider='dashscope', model='text-embedding-v1')\n","\n","created = build_embeddings_and_vectorstores(\n","    chunks, username=username, character_names=character_names, persist_dir=str(INDEX_DIR), emb_cfg=emb_cfg\n",")\n","print('[INFO] Collections created:', created)"]},{"cell_type":"code","execution_count":null,"id":"yXFa31-dfMwa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXFa31-dfMwa","outputId":"af01fe72-9e9e-40d7-c28e-584339e89eda","executionInfo":{"status":"ok","timestamp":1759731981186,"user_tz":300,"elapsed":15,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… vectorstore.py patched & reloaded.\n"]}],"source":["from pathlib import Path\n","import textwrap, importlib, sys, json\n","\n","PROJ = Path('/content/drive/MyDrive/rag_bio_project').resolve()\n","SRC = PROJ/'src'\n","assert SRC.exists(), f\"src not found: {SRC}\"\n","\n","vf = SRC/'vectorstore.py'\n","vf.write_text(textwrap.dedent(r'''\n","from chromadb import PersistentClient\n","import json\n","\n","# We reuse the same embedder used for indexing\n","from embeddings import EmbeddingConfig, build_embeddings\n","\n","def _embedder_from_coll_meta(meta: dict):\n","    info = meta.get(\"embedding\")\n","    if isinstance(info, str):\n","        try:\n","            info = json.loads(info)\n","        except Exception:\n","            info = {}\n","    info = info or {}\n","    provider  = (info.get(\"provider\") or \"hf\")\n","    model     = (info.get(\"model\") or \"BAAI/bge-small-zh-v1.5\")\n","    normalize = bool(info.get(\"normalize\", True))\n","    return build_embeddings(EmbeddingConfig(provider=provider, model=model, normalize=normalize))\n","\n","def get_collection(persist_dir: str, collection_name: str):\n","    client = PersistentClient(path=persist_dir)\n","    return client.get_collection(collection_name)\n","\n","def quick_query(persist_dir: str, collection_name: str, query_text: str, n_results: int = 5):\n","    client = PersistentClient(path=persist_dir)\n","    coll = client.get_collection(collection_name)\n","    embedder = _embedder_from_coll_meta(coll.metadata or {})\n","    # Compute query embedding with the SAME model/dim as the collection\n","    if hasattr(embedder, \"embed_query\"):\n","        qvec = embedder.embed_query(query_text)\n","    else:\n","        qvec = embedder.embed_documents([query_text])[0]\n","    return coll.query(query_embeddings=[qvec], n_results=n_results,\n","                      include=[\"documents\",\"metadatas\",\"distances\"])\n","'''), encoding='utf-8')\n","\n","# çƒ­é‡è½½\n","sys.path.insert(0, str(SRC))\n","import vectorstore\n","import importlib\n","importlib.reload(vectorstore)\n","from vectorstore import quick_query\n","print(\"âœ… vectorstore.py patched & reloaded.\")\n"]},{"cell_type":"code","source":["# from chromadb import PersistentClient\n","\n","# def debug_query_exact(persist_dir, collection_name, query, n_results=3):\n","#     print(\"[DEBUG] persist_dir:\", persist_dir)\n","#     client = PersistentClient(path=str(persist_dir))\n","#     names = [c.name for c in client.list_collections()]\n","#     print(\"[DEBUG] collections:\", names)\n","\n","#     if collection_name not in names:\n","#         raise ValueError(f\"'{collection_name}' not found in this directory. Check the path and name above.\")\n","\n","#     coll = client.get_collection(collection_name)\n","#     print(\"[DEBUG] dim:\", coll.metadata.get(\"dimension\", \"?\"), \"embed:\", coll.metadata.get(\"embedding\", \"?\"))\n","#     out = coll.query(query_texts=[query], n_results=n_results, include=[\"documents\",\"metadatas\",\"distances\"])\n","#     return out\n","\n","# # ç”¨ä½ åˆšæ‰é‡å»ºæ—¶çš„ç»å¯¹è·¯å¾„ + åˆ—è¡¨é‡Œçœ‹åˆ°çš„ç¡®åˆ‡é›†åˆå\n","# res = debug_query_exact(\n","#     persist_dir=\"/content/drive/MyDrive/rag_bio_project/index\",  # æˆ–è€… index_m3\n","#     collection_name=\"demo_user_LiQing\",\n","#     query=\"income and marriage\",\n","#     n_results=3\n","# )\n"],"metadata":{"id":"xQgMkSvrIs9z"},"id":"xQgMkSvrIs9z","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"EnS2XDXdaSrS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnS2XDXdaSrS","outputId":"96c70872-6ddd-4a3a-d798-02a662b742cc","executionInfo":{"status":"ok","timestamp":1759731993619,"user_tz":300,"elapsed":12411,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["\n","[TEST] Query LiQing collection:\n","[INFO] Using HF embeddings: BAAI/bge-m3 (normalize=True)\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------\n","rank 1 dist 1.0328842980933313\n","source data_txt/wangmu.txt\n","Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations. Finance: ~200k CNY income; house + money-market funds; conservative. Marriage: single; passionate about education equity. Experience\n","------------------------------------------------------------\n","rank 2 dist 1.0494898525093794\n","source data_txt/liqing.txt\n","Born in Suzhou in 1990; studied Economics and became a product manager. Finance: ~350k CNY income; index funds; medium risk preference. Marriage: married in 2018; one daughter in 2022. Experience: 202\n","------------------------------------------------------------\n","rank 3 dist 1.1080089675205367\n","source /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf\n","Li Qing Biography Born in Suzhou in 1990; studied Economics and became a product manager. Finance: ~350k CNY income; index funds; medium risk preference. Marriage: married in 2018; one daughter in 202\n","\n","[TEST] Query WangMu collection:\n","[INFO] Using HF embeddings: BAAI/bge-m3 (normalize=True)\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["------------------------------------------------------------\n","rank 1 dist 1.0690570179670293\n","source data_txt/wangmu.txt\n","Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations. Finance: ~200k CNY income; house + money-market funds; conservative. Marriage: single; passionate about education equity. Experience\n","------------------------------------------------------------\n","rank 2 dist 1.1543111426137778\n","source data_txt/liqing.txt\n","Born in Suzhou in 1990; studied Economics and became a product manager. Finance: ~350k CNY income; index funds; medium risk preference. Marriage: married in 2018; one daughter in 2022. Experience: 202\n","------------------------------------------------------------\n","rank 3 dist 1.1847435711444114\n","source /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf\n","Li Qing Biography Born in Suzhou in 1990; studied Economics and became a product manager. Finance: ~350k CNY income; index funds; medium risk preference. Marriage: married in 2018; one daughter in 202\n"]}],"source":["print('\\n[TEST] Query LiQing collection:')\n","res = quick_query(str(INDEX_DIR), 'demo_user_LiQing', 'income and marriage', n_results=3)\n","for i,(doc,meta,dist) in enumerate(zip(res.get('documents',[['']])[0], res.get('metadatas',[['']])[0], res.get('distances',[['']])[0])):\n","    print('-'*60)\n","    print('rank', i+1, 'dist', dist)\n","    print('source', meta.get('source'))\n","    print(doc[:200].replace('\\n',' '))\n","\n","print('\\n[TEST] Query WangMu collection:')\n","res = quick_query(str(INDEX_DIR), 'demo_user_WangMu', 'risk control and marriage', n_results=3)\n","for i,(doc,meta,dist) in enumerate(zip(res.get('documents',[['']])[0], res.get('metadatas',[['']])[0], res.get('distances',[['']])[0])):\n","    print('-'*60)\n","    print('rank', i+1, 'dist', dist)\n","    print('source', meta.get('source'))\n","    print(doc[:200].replace('\\n',' '))"]},{"cell_type":"markdown","id":"7adf997c","metadata":{"id":"7adf997c"},"source":["# ğŸ”§ RAG Pipeline â€” Retriever â†’ Middleware â†’ Prompt â†’ LLM (Appended)\n","ä»¥ä¸‹å•å…ƒæ ¼åŸºäºä½ ç°æœ‰å·¥ç¨‹ï¼Œè¡¥å……å®Œæ•´çš„æ£€ç´¢ä¸­é—´å±‚ä¸æµæ°´çº¿æµ‹è¯•ï¼Œå¹¶å¯ä¸€é”®è·‘é€š Demoã€‚"]},{"cell_type":"code","execution_count":null,"id":"UrBSPRJujqaX","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UrBSPRJujqaX","outputId":"3d33e494-7402-45fc-aa13-2d6b69553d92","executionInfo":{"status":"ok","timestamp":1759731993638,"user_tz":300,"elapsed":3,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["name: demo_user_LiQing | id: c5f3f449-cc0c-4059-aa66-007c0d0e4362 | count: 4\n","meta: {'embedding': '{\"provider\": \"hf\", \"model\": \"BAAI/bge-m3\", \"dim\": 1024, \"normalize\": true}'}\n","------------------------------------------------------------\n","name: demo_user_WangMu | id: dd13a5aa-1d1d-4991-8359-f70c64ad522d | count: 4\n","meta: {'embedding': '{\"provider\": \"hf\", \"model\": \"BAAI/bge-m3\", \"dim\": 1024, \"normalize\": true}'}\n","------------------------------------------------------------\n"]}],"source":["from chromadb import PersistentClient\n","client = PersistentClient(path=\"/content/drive/MyDrive/rag_bio_project/index\")\n","\n","for col in client.list_collections():\n","    coll = client.get_collection(col.name)\n","    print(\"name:\", coll.name, \"| id:\", coll.id, \"| count:\", coll.count())\n","    print(\"meta:\", coll.metadata)\n","    print(\"-\"*60)\n"]},{"cell_type":"code","execution_count":null,"id":"n8NRBSSL29Az","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8NRBSSL29Az","outputId":"2ed51655-fc8d-4871-84b2-cef9ab2d0540","executionInfo":{"status":"ok","timestamp":1759731996052,"user_tz":300,"elapsed":2413,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[.env keys] ['OPENAI_API_KEY']\n","[OPENAI_API_KEY startswith sk?] sk-proj\n","[verify] OPENAI_API_KEY set: True\n"]}],"source":["%pip -q install python-dotenv\n","\n","from pathlib import Path\n","from dotenv import load_dotenv, dotenv_values\n","import os\n","\n","PROJ = Path(\"/content/drive/MyDrive/rag_bio_project\")  # æŒ‰ä½ çš„çœŸå®è·¯å¾„\n","ENV_PATH = PROJ / \".env\"\n","\n","# è¯»å–çœ‹ä¸€ä¸‹æ˜¯å¦çœŸçš„æ‹¿åˆ° key\n","cfg = dotenv_values(ENV_PATH)      # åªè¯»å–ï¼Œä¸å†™ç¯å¢ƒ\n","print(\"[.env keys]\", list(cfg.keys()))\n","print(\"[OPENAI_API_KEY startswith sk?]\", str(cfg.get(\"OPENAI_API_KEY\",\"\"))[:7])\n","\n","# çœŸæ­£å†™å…¥å½“å‰è¿›ç¨‹çš„ç¯å¢ƒå˜é‡ï¼ˆoverride=True è¦†ç›–å·²æœ‰å€¼ï¼‰\n","load_dotenv(ENV_PATH, override=True)\n","\n","print(\"[verify] OPENAI_API_KEY set:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"]},{"cell_type":"code","execution_count":null,"id":"30838bc7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30838bc7","outputId":"098aae75-af8d-4b66-ab9a-666db67ca7eb","executionInfo":{"status":"ok","timestamp":1759732000470,"user_tz":300,"elapsed":4416,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deps OK\n"]}],"source":["\n","# 0) å®‰è£…ä¾èµ–ï¼ˆå¦‚å·²å®‰è£…å¯è·³è¿‡ï¼‰\n","%pip -q install -U numpy==1.26.4 chromadb==0.4.24 langchain==0.2.11 langchain-core==0.2.26         langchain-community==0.2.10 langchain-openai==0.1.17 pypdf tiktoken\n","# %pip -q install -U langchain-ollama  # å¦‚æœè¦ç”¨æœ¬åœ° Ollama æ¨¡å‹\n","print(\"Deps OK\")\n"]},{"cell_type":"code","execution_count":null,"id":"12abb7f8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12abb7f8","outputId":"9710ee8b-a96b-4965-e31f-3a7e6dd0dc89","executionInfo":{"status":"ok","timestamp":1759732000490,"user_tz":300,"elapsed":7,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Project: /content/drive/MyDrive/rag_bio_project\n","Index: /content/drive/MyDrive/rag_bio_project/index\n"]}],"source":["\n","# 1) è·¯å¾„è®¾ç½®ï¼šæŒ‡å‘æˆ‘ä»¬åˆšåˆšç”Ÿæˆçš„å·¥ç¨‹ç›®å½•\n","from pathlib import Path\n","PROJ = Path(\"/content/drive/MyDrive/rag_bio_project\")\n","SRC = PROJ/\"src\"\n","INDEX_DIR = PROJ/\"index\"     # æŒ‡å‘ä½ çš„ Chroma ç´¢å¼•ç›®å½•ï¼ˆDrive ä¸­åŒåä¹Ÿå¯æ”¹è¿™é‡Œï¼‰\n","import sys\n","sys.path.append(str(SRC))\n","print(\"Project:\", PROJ)\n","print(\"Index:\", INDEX_DIR)\n"]},{"cell_type":"code","execution_count":null,"id":"593ebeb0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"593ebeb0","outputId":"69a78d0a-d6b7-4cc6-d439-7927b2c36715","executionInfo":{"status":"ok","timestamp":1759732000493,"user_tz":300,"elapsed":3,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["Collections: ['demo_user_LiQing', 'demo_user_WangMu']\n"]}],"source":["\n","# 2) åˆ—å‡ºå‘é‡åº“ collections ä»¥ç¡®è®¤å¯è§\n","from chromadb import PersistentClient\n","client = PersistentClient(path=str(INDEX_DIR))\n","cols = [c.name for c in client.list_collections()]\n","print(\"Collections:\", cols)\n"]},{"cell_type":"markdown","id":"d128a568","metadata":{"id":"d128a568"},"source":["## âœ… Retriever å†’çƒŸæµ‹è¯•ï¼ˆé«˜é˜ˆå€¼ + MMR + åˆ†çº§ï¼‰"]},{"cell_type":"code","execution_count":null,"id":"R_letEGl37Nk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_letEGl37Nk","outputId":"99eb40cf-da0b-4d85-a8ec-d39e9abd0047","executionInfo":{"status":"ok","timestamp":1759732000513,"user_tz":300,"elapsed":19,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"]},{"output_type":"stream","name":"stdout","text":["INDEX_DIR = /content/drive/MyDrive/rag_bio_project/index\n","Collections: ['demo_user_LiQing', 'demo_user_WangMu']\n"," - demo_user_LiQing                 count=4  embedding={'provider': 'hf', 'model': 'BAAI/bge-m3', 'dim': 1024, 'normalize': True}\n"," - demo_user_WangMu                 count=4  embedding={'provider': 'hf', 'model': 'BAAI/bge-m3', 'dim': 1024, 'normalize': True}\n"]}],"source":["from pathlib import Path\n","from chromadb import PersistentClient\n","import json\n","\n","print(\"INDEX_DIR =\", INDEX_DIR)\n","client = PersistentClient(path=str(INDEX_DIR))\n","names = [c.name for c in client.list_collections()]\n","print(\"Collections:\", names)\n","\n","for n in names:\n","    coll = client.get_collection(n)\n","    meta = coll.metadata or {}\n","    emb = meta.get(\"embedding\")\n","    if isinstance(emb, str):\n","        try: emb = json.loads(emb)\n","        except: pass\n","    print(f\" - {n:<32} count={coll.count()}  embedding={emb}\")\n"]},{"cell_type":"code","execution_count":null,"id":"g7cTmMlX4S_E","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7cTmMlX4S_E","outputId":"eefba9b2-7150-4cee-878c-93b06b092826","executionInfo":{"status":"ok","timestamp":1759732014179,"user_tz":300,"elapsed":13665,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n"]},{"output_type":"stream","name":"stdout","text":["raw scores (top 10): [0.4168, 0.4168, 0.409, 0.409, 0.3638, 0.3638, 0.2272, 0.2272]\n"]}],"source":["from retriever import retrieve\n","\n","tmp = retrieve(\n","    persist_dir=str(INDEX_DIR),\n","    query_text=\"æé’çš„å¹´æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ\",\n","    k=10, strategy=\"similarity\",\n","    fetch_k=50,\n","    score_threshold=0.0,     # å…³é—­é˜ˆå€¼\n",")\n","\n","scores = [round(x[\"score\"], 4) for x in tmp.get(\"items\", [])]\n","print(\"raw scores (top 10):\", scores[:10])\n"]},{"cell_type":"code","execution_count":null,"id":"UpU3dDMN4cc4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UpU3dDMN4cc4","outputId":"185d3ae0-325b-4acc-f4bf-5ccd1591784a","executionInfo":{"status":"ok","timestamp":1759732020093,"user_tz":300,"elapsed":5904,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"]},{"output_type":"stream","name":"stdout","text":["docs: ['Li Qing Biography\\nBorn in Suzhou in 1990; studied Economics and became a product manager.\\nFinance: ~350k CNY income; index funds; medium risk preference.\\nMarriage: married in 2018; one daughter in 2022.\\nExperience: 2020-2022 SEA market localization projects.', 'Born in Chengdu in 1985; LL.B then MFin; shifted to NGO operations.\\nFinance: ~200k CNY income; house + money-market funds; conservative.\\nMarriage: single; passionate about education equity.\\nExperience: 2015-2019 broker risk control; compliance expertise.']\n","dists: [1.0725657381969864, 1.0747644230923041]\n"]}],"source":["from chromadb import PersistentClient\n","from retriever import _embedder_from_coll_meta  # æˆ‘ä»¬ä»£ç é‡Œå·²æœ‰\n","\n","client = PersistentClient(path=str(INDEX_DIR))\n","# æŒ‘ä¸€ä¸ªä½ ç¡®è®¤æœ‰â€œæé’â€å†…å®¹çš„é›†åˆå\n","cname = [n for n in names if \"LiQing\" in n or \"liqing\" in n.lower()][0]\n","coll = client.get_collection(cname)\n","\n","embedder = _embedder_from_coll_meta(coll.metadata or {})\n","q = \"æé’ å¹´ æ”¶å…¥\"\n","qvec = (embedder.embed_query(q)\n","        if hasattr(embedder, \"embed_query\")\n","        else embedder.embed_documents([q])[0])\n","\n","qr = coll.query(query_embeddings=[qvec], n_results=5,\n","                include=[\"documents\",\"distances\",\"metadatas\"])\n","print(\"docs:\", qr[\"documents\"][0][:2])\n","print(\"dists:\", qr[\"distances\"][0][:2])\n"]},{"cell_type":"code","execution_count":null,"id":"599dd73b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"599dd73b","outputId":"b54a5e16-fa49-4ed2-d533-6aca9317a147","executionInfo":{"status":"ok","timestamp":1759732031539,"user_tz":300,"elapsed":11445,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 4, updating n_results = 4\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 4, updating n_results = 4\n"]},{"output_type":"stream","name":"stdout","text":["Route: None\n"]}],"source":["\n","from retriever import retrieve\n","question = \"æé’çš„å¹´æ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ\"\n","res = retrieve(persist_dir=str(INDEX_DIR), query_text=question,\n","               k=5, strategy=\"mmr\", strictness=\"strict\")\n","print(\"Route:\", res.get(\"route\"))\n","for it in res.get(\"items\", [])[:3]:\n","    print(it[\"grade\"], f\"{it['score']:.3f}\", \"src:\", (it[\"metadata\"] or {}).get(\"source\"))\n"]},{"cell_type":"markdown","id":"9b83ff81","metadata":{"id":"9b83ff81"},"source":["## ğŸ§© Middlewareï¼šè§’è‰²è¯†åˆ« / æ€€ç–‘åº¦ï¼ˆå ä½ï¼‰ / äººæ ¼ï¼ˆå ä½ï¼‰ / å¤æŸ¥ï¼ˆå ä½ï¼‰"]},{"cell_type":"code","execution_count":null,"id":"5738f107","metadata":{"id":"5738f107"},"outputs":[],"source":["\n","# from middleware import detect_characters_from_question, VerificationConfig, verify_answer_against_context\n","# det = detect_characters_from_question(\"è¯·æ¯”è¾ƒLiQingä¸WangMuçš„æ”¶å…¥\", persist_dir=str(INDEX_DIR))\n","# print(\"Role detection:\", det)\n","# # å¤æŸ¥å ä½æ¼”ç¤ºï¼ˆé»˜è®¤ä¸å¯ç”¨ï¼‰\n","# vres = verify_answer_against_context(\"dummy answer\", res.get(\"items\", []), VerificationConfig(enabled=False))\n","# print(\"Verification (disabled):\", vres)\n"]},{"cell_type":"markdown","id":"934592ca","metadata":{"id":"934592ca"},"source":["## ğŸ§  Prompt è‡ªåŠ¨é€‰æ‹© + é¢„è§ˆ"]},{"cell_type":"code","execution_count":null,"id":"025628fa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"025628fa","outputId":"e5316fa7-94cf-4ac5-ee91-a8e0d76c2232","executionInfo":{"status":"ok","timestamp":1759732031558,"user_tz":300,"elapsed":11,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Prompt mode=rag_concise, context_len={len(ctx)}\n","Mode: rag_concise\n","System msg preview: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say you d\n"]}],"source":["\n","from prompting import build_prompt_messages_auto\n","msgs, info = build_prompt_messages_auto(\"What is LiQing's annual income?\", res)\n","print(\"Mode:\", info[\"mode\"])\n","print(\"System msg preview:\", msgs[0].content[:160])\n"]},{"cell_type":"markdown","id":"e029bff5","metadata":{"id":"e029bff5"},"source":["## ğŸš€ ä¸€é”®æµæ°´çº¿ Demoï¼ˆéœ€è¦ API Keyï¼‰"]},{"cell_type":"code","execution_count":null,"id":"34c4db4b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34c4db4b","outputId":"dc8d28bb-1bd9-47f7-cd13-99452b133320","executionInfo":{"status":"ok","timestamp":1759732085825,"user_tz":300,"elapsed":13335,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Prompt mode=rag_concise, context_len={len(ctx)}\n","Answer: Li Qing's annual income is approximately 350,000 CNY.\n","\n","References:\n"," [1] /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf Â· p.0\n","[2] /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf Â· p.0\n","\n","Prompt mode: rag_concise\n","Route: {'collection': 'demo_user_LiQing', 'username': 'demo', 'character': 'user_LiQing'}\n"]}],"source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # å»ºè®®å…ˆç”¨ mediumï¼Œç¡®è®¤å¬å›åå†è°ƒé«˜\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # å¯é€‰ï¼šåœ¨è¿™é‡Œå¼€å¯/è°ƒæ•´ä¸­é—´å±‚\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"What's LiQing's annual income?\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"]},{"cell_type":"code","source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # å»ºè®®å…ˆç”¨ mediumï¼Œç¡®è®¤å¬å›åå†è°ƒé«˜\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # å¯é€‰ï¼šåœ¨è¿™é‡Œå¼€å¯/è°ƒæ•´ä¸­é—´å±‚\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"LiQing, what's your annual income?\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vO09NNhgNfTD","executionInfo":{"status":"ok","timestamp":1759732305949,"user_tz":300,"elapsed":13242,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}},"outputId":"64fa0a96-168d-4e67-8752-009f6e326c7d"},"id":"vO09NNhgNfTD","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Prompt mode=rag_concise, context_len={len(ctx)}\n","Answer: Li Qing's annual income is approximately 350,000 CNY.\n","\n","References:\n"," [1] /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf Â· p.0\n","[2] /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf Â· p.0\n","\n","Prompt mode: rag_concise\n","Route: {'collection': 'demo_user_LiQing', 'username': 'demo', 'character': 'user_LiQing'}\n"]}]},{"cell_type":"code","source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # å»ºè®®å…ˆç”¨ mediumï¼Œç¡®è®¤å¬å›åå†è°ƒé«˜\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # å¯é€‰ï¼šåœ¨è¿™é‡Œå¼€å¯/è°ƒæ•´ä¸­é—´å±‚\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"LiQing, how's your dog?\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FBYVSlbNj_n","executionInfo":{"status":"ok","timestamp":1759733753833,"user_tz":300,"elapsed":11721,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}},"outputId":"a54fa69b-e97c-4769-9391-5298fea00335"},"id":"6FBYVSlbNj_n","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Prompt mode=rag_concise, context_len={len(ctx)}\n","Answer: I don't know.\n","\n","References:\n"," \n","\n","Prompt mode: rag_concise\n","Route: None\n"]}]},{"cell_type":"code","source":["from pipeline import PipelineConfig, run_pipeline\n","# from middleware import DeceptionConfig\n","\n","cfg = PipelineConfig(\n","    persist_dir=str(INDEX_DIR),\n","    strictness=\"medium\",              # å»ºè®®å…ˆç”¨ mediumï¼Œç¡®è®¤å¬å›åå†è°ƒé«˜\n","    provider=\"openai\", model=\"gpt-4o-mini\", temperature=0.2,\n","    do_role_detection=True,\n","    # å¯é€‰ï¼šåœ¨è¿™é‡Œå¼€å¯/è°ƒæ•´ä¸­é—´å±‚\n","    # suspicion=SuspicionConfig(enabled=True, level=0.0),\n","    # verification=VerificationConfig(enabled=True, mode=\"presence\", min_hits=1),\n","    # deception=DeceptionConfig(enabled=False),\n",")\n","\n","# print(\"deception enabled?\", cfg.deception.enabled)  # False\n","out = run_pipeline(\"LiQing, ä½ çš„å¹´æ”¶å…¥å¤šå°‘?\", cfg)\n","print(\"Answer:\", out[\"answer\"][:500])\n","print(\"\\nReferences:\\n\", out[\"references\"])\n","print(\"\\nPrompt mode:\", out[\"prompt_mode\"])\n","print(\"Route:\", out[\"route\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJK5Ywm6TeZ6","executionInfo":{"status":"ok","timestamp":1759733766029,"user_tz":300,"elapsed":12188,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}},"outputId":"09087c88-a1f6-4746-a8c5-86c4231a9d8e"},"id":"lJK5Ywm6TeZ6","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n","WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 4, updating n_results = 4\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Prompt mode=rag_concise, context_len={len(ctx)}\n","Answer: Li Qing's annual income is approximately 350,000 CNY.\n","\n","References:\n"," [1] /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf Â· p.0\n","[2] /content/drive/MyDrive/rag_bio_project/data_pdfs/liqing.pdf Â· p.0\n","\n","Prompt mode: rag_concise\n","Route: {'collection': 'demo_user_LiQing', 'username': 'demo', 'character': 'user_LiQing'}\n"]}]},{"cell_type":"code","execution_count":null,"id":"upaIbVgo-5lf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upaIbVgo-5lf","outputId":"65febf20-a9b7-4374-f05e-1c9aa716c2f7","executionInfo":{"status":"ok","timestamp":1759732072374,"user_tz":300,"elapsed":5,"user":{"displayName":"Tony Xu","userId":"05691063796905587196"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["OPENAI_API_KEY exists: True\n","OPENAI_ORG: None\n"]}],"source":["import os\n","print(\"OPENAI_API_KEY exists:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n","print(\"OPENAI_ORG:\", os.getenv(\"OPENAI_ORG\"))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":5}